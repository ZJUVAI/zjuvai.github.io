<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.3.1">

  

  
  
  
  
  
  
  
  
  
  

  
  <meta name="author" content="Minfeng Zhu">

  
  
  
  
  
  <meta name="description" content="The surge in Large Language Models (LLMs) has revolutionized natural language processing, but fine-tuning them for specific tasks often encounters challenges in balancing performance and preserving general instruction-following abilities. In this paper, we posit that the distribution gap between task datasets and the LLMs serves as the primary underlying cause. To address the problem, we introduce Self-Distillation Fine-Tuning (SDFT), a novel approach that bridges the distribution gap by guiding fine-tuning with a distilled dataset generated by the model itself to match its original distribution. Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs. Our code is available at [this https URL](https://github.com/sail-sg/sdft).">

  
  <link rel="alternate" hreflang="en-us" href="/publications/self-distillation-bridges-distribution-gap-in-language-model-fine-tuning/">

  


  

  
  
  
  <meta name="theme-color" content="#546787">
  

  
  
  
  
  
  <link rel="stylesheet" href="/js/academicons/1.9.0/css/academicons.min.css" integrity="" crossorigin="anonymous">
  <link rel="stylesheet" href="/js/font-awesome/5.14.0/css/all.min.css" integrity="" crossorigin="anonymous">
  <link rel="stylesheet" href="/js/fancybox/3.5.7/jquery.fancybox.min.css" integrity="" crossorigin="anonymous">

  
  
  
  
  
  
  
  
  
  <link rel="stylesheet" href="/js/highlight.js/10.1.2/styles/default.min.css" crossorigin="anonymous" title="hl-light">
  <link rel="stylesheet" href="/js/highlight.js/10.1.2/styles/default.min.css" crossorigin="anonymous" title="hl-dark" disabled>
  
  
  

  

  

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.eb16c3d626b5f258fcafbef695c15572.css">

  

  
  
  

  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/avatar.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/avatar.png">

  <link rel="canonical" href="/publications/self-distillation-bridges-distribution-gap-in-language-model-fine-tuning/">

  
  
  
  
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="ZJU VAI">
  <meta property="og:url" content="/publications/self-distillation-bridges-distribution-gap-in-language-model-fine-tuning/">
  <meta property="og:title" content="Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning. | ZJU VAI">
  <meta property="og:description" content="The surge in Large Language Models (LLMs) has revolutionized natural language processing, but fine-tuning them for specific tasks often encounters challenges in balancing performance and preserving general instruction-following abilities. In this paper, we posit that the distribution gap between task datasets and the LLMs serves as the primary underlying cause. To address the problem, we introduce Self-Distillation Fine-Tuning (SDFT), a novel approach that bridges the distribution gap by guiding fine-tuning with a distilled dataset generated by the model itself to match its original distribution. Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs. Our code is available at [this https URL](https://github.com/sail-sg/sdft).">
  <meta property="og:image" content="/publications/self-distillation-bridges-distribution-gap-in-language-model-fine-tuning/featured.png">
  <meta property="og:locale" content="en-us">
  
  
  <meta property="article:published_time" content="2024-05-28T00:00:00&#43;00:00">
  
  
  <meta property="article:modified_time" content="2024-05-28T00:00:00&#43;00:00">
  
  

  


  





  <title>Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning. | ZJU VAI</title>

</head>


<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">ZJU VAI</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/blog"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/members"><span>Members</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/publications"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/talks"><span>Talks</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


  <div class="pub" itemscope itemtype="http://schema.org/CreativeWork">

  













<div class="article-header d-xl-none">
  <div class="featured-image" style="background-image: url('/publications/self-distillation-bridges-distribution-gap-in-language-model-fine-tuning/featured.png');"></div>
  
</div>


<div class="container-fluid split-header d-none d-xl-block">
  <div class="row">
    <div class="col-6">
      <div class="split-header-content">
        <h1 itemprop="name">Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning.</h1>

        

        



<meta content="2024-05-28 00:00:00 &#43;0000 UTC" itemprop="datePublished">
<meta content="2024-05-28 00:00:00 &#43;0000 UTC" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



<span itemprop="author name" itemtype="http://schema.org/Person">
  
  

  
  
  <a href="/authors/zhaoruiyang" style="text-decoration: underline;">Zhaorui Yang</a>
  </span>, <span itemprop="author name" itemtype="http://schema.org/Person">
  
  

  
  
  Tianyu Pang
  </span>, <span itemprop="author name" itemtype="http://schema.org/Person">
  
  

  
  
  <a href="/authors/haozhefeng" style="text-decoration: underline;">Haozhe Feng</a>
  </span>, <span itemprop="author name" itemtype="http://schema.org/Person">
  
  

  
  
  Han Wang
  </span>, <span itemprop="author name" itemtype="http://schema.org/Person">
  
  

  
  
  <a href="/authors/weichen" style="text-decoration: underline;">Wei Chen</a>
  </span>, <span itemprop="author name" itemtype="http://schema.org/Person">
  
  

  
  
  <a href="/authors/minfengzhu" style="text-decoration: underline;">Minfeng Zhu</a>
  </span>, <span itemprop="author name" itemtype="http://schema.org/Person">
  
  

  
  
  Qian Liu
  </span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    <time>May 28, 2024</time>
  </span>
  

  

  

  
  

  
  

  

</div>


        













<div class="btn-links mb-3">
  
  










    
      <a class="btn btn-outline-primary my-1 mr-1" href="https://arxiv.org/pdf/2402.13669" target="_blank" rel="noopener">
      PDF
      </a>
    






    






    






    






    






    






    





    





















<a class="btn btn-outline-primary my-1 mr-1" href="https://doi.org/10.48550/arXiv.2402.13669"
  target="_blank" rel="noopener">
  DOI
</a>




</div>



        
<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=/publications/self-distillation-bridges-distribution-gap-in-language-model-fine-tuning/&amp;text=Self-Distillation%20Bridges%20Distribution%20Gap%20in%20Language%20Model%20Fine-Tuning." target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=/publications/self-distillation-bridges-distribution-gap-in-language-model-fine-tuning/&amp;t=Self-Distillation%20Bridges%20Distribution%20Gap%20in%20Language%20Model%20Fine-Tuning." target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook-f"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Self-Distillation%20Bridges%20Distribution%20Gap%20in%20Language%20Model%20Fine-Tuning.&amp;body=/publications/self-distillation-bridges-distribution-gap-in-language-model-fine-tuning/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=/publications/self-distillation-bridges-distribution-gap-in-language-model-fine-tuning/&amp;title=Self-Distillation%20Bridges%20Distribution%20Gap%20in%20Language%20Model%20Fine-Tuning." target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Self-Distillation%20Bridges%20Distribution%20Gap%20in%20Language%20Model%20Fine-Tuning.%20/publications/self-distillation-bridges-distribution-gap-in-language-model-fine-tuning/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=/publications/self-distillation-bridges-distribution-gap-in-language-model-fine-tuning/&amp;title=Self-Distillation%20Bridges%20Distribution%20Gap%20in%20Language%20Model%20Fine-Tuning." target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>


      </div>
    </div>
    <div class="col-6">
      <div class="split-header-image">
        <img src="/publications/self-distillation-bridges-distribution-gap-in-language-model-fine-tuning/featured.png" itemprop="image" alt="">
        
      </div>
    </div>
  </div>
</div>

<div class="article-container d-xl-none">
  <h1 itemprop="name">Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning.</h1>

  

  



<meta content="2024-05-28 00:00:00 &#43;0000 UTC" itemprop="datePublished">
<meta content="2024-05-28 00:00:00 &#43;0000 UTC" itemprop="dateModified">

<div class="article-metadata">

  
  
  
  
  <div>
    



<span itemprop="author name" itemtype="http://schema.org/Person">
  
  

  
  
  <a href="/authors/zhaoruiyang" style="text-decoration: underline;">Zhaorui Yang</a>
  </span>, <span itemprop="author name" itemtype="http://schema.org/Person">
  
  

  
  
  Tianyu Pang
  </span>, <span itemprop="author name" itemtype="http://schema.org/Person">
  
  

  
  
  <a href="/authors/haozhefeng" style="text-decoration: underline;">Haozhe Feng</a>
  </span>, <span itemprop="author name" itemtype="http://schema.org/Person">
  
  

  
  
  Han Wang
  </span>, <span itemprop="author name" itemtype="http://schema.org/Person">
  
  

  
  
  <a href="/authors/weichen" style="text-decoration: underline;">Wei Chen</a>
  </span>, <span itemprop="author name" itemtype="http://schema.org/Person">
  
  

  
  
  <a href="/authors/minfengzhu" style="text-decoration: underline;">Minfeng Zhu</a>
  </span>, <span itemprop="author name" itemtype="http://schema.org/Person">
  
  

  
  
  Qian Liu
  </span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    <time>May 28, 2024</time>
  </span>
  

  

  

  
  

  
  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=/publications/self-distillation-bridges-distribution-gap-in-language-model-fine-tuning/&amp;text=Self-Distillation%20Bridges%20Distribution%20Gap%20in%20Language%20Model%20Fine-Tuning." target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=/publications/self-distillation-bridges-distribution-gap-in-language-model-fine-tuning/&amp;t=Self-Distillation%20Bridges%20Distribution%20Gap%20in%20Language%20Model%20Fine-Tuning." target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook-f"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Self-Distillation%20Bridges%20Distribution%20Gap%20in%20Language%20Model%20Fine-Tuning.&amp;body=/publications/self-distillation-bridges-distribution-gap-in-language-model-fine-tuning/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=/publications/self-distillation-bridges-distribution-gap-in-language-model-fine-tuning/&amp;title=Self-Distillation%20Bridges%20Distribution%20Gap%20in%20Language%20Model%20Fine-Tuning." target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Self-Distillation%20Bridges%20Distribution%20Gap%20in%20Language%20Model%20Fine-Tuning.%20/publications/self-distillation-bridges-distribution-gap-in-language-model-fine-tuning/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=/publications/self-distillation-bridges-distribution-gap-in-language-model-fine-tuning/&amp;title=Self-Distillation%20Bridges%20Distribution%20Gap%20in%20Language%20Model%20Fine-Tuning." target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>


  

</div>

  













<div class="btn-links mb-3">
  
  










    
      <a class="btn btn-outline-primary my-1 mr-1" href="https://arxiv.org/pdf/2402.13669" target="_blank" rel="noopener">
      PDF
      </a>
    






    






    






    






    






    






    





    





















<a class="btn btn-outline-primary my-1 mr-1" href="https://doi.org/10.48550/arXiv.2402.13669"
  target="_blank" rel="noopener">
  DOI
</a>




</div>


</div>



  <div class="article-container">

    
    <h3>Abstract</h3>
    <p class="pub-abstract" itemprop="text">The surge in Large Language Models (LLMs) has revolutionized natural language processing, but fine-tuning them for specific tasks often encounters challenges in balancing performance and preserving general instruction-following abilities. In this paper, we posit that the distribution gap between task datasets and the LLMs serves as the primary underlying cause. To address the problem, we introduce Self-Distillation Fine-Tuning (SDFT), a novel approach that bridges the distribution gap by guiding fine-tuning with a distilled dataset generated by the model itself to match its original distribution. Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs. Our code is available at <a href="https://github.com/sail-sg/sdft">this https URL</a>.</p>
    

    
    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Type</div>
          <div class="col-12 col-md-9">
            
            
            <a href="#1">
              Conference paper
            </a>
            
          </div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>
    

    
    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Publication</div>
          <div class="col-12 col-md-9">The 62nd Annual Meeting of the Association for Computational Linguistics</div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>
    

    <div class="space-below"></div>

    <div class="article-style"></div>

    

<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/large-language-models/">large language models</a>
  
</div>


    








  
  
    
  
  





  
  
  
  
  <div class="media author-card" itemscope itemtype="http://schema.org/Person">
    

    <div class="media-body">
      <h5 class="card-title" itemprop="name"><a href="/authors/zhaorui-yang/"></a></h5>
      
      
      <ul class="network-icon" aria-hidden="true">
        
      </ul>
    </div>
  </div>




  </div>
</div>



      

    
    

    
    
    
    <script src="/js/jquery/3.5.1/jquery.min.js" integrity="" crossorigin="anonymous"></script>
    <script src="/js/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="" crossorigin="anonymous"></script>
    <script src="/js/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="" crossorigin="anonymous"></script>
    <script src="/js/fancybox/3.5.7/jquery.fancybox.min.js" integrity="" crossorigin="anonymous"></script>

    

    
    
    <script src="/js/highlight.js/10.1.2/highlight.min.js" integrity="" crossorigin="anonymous"></script>
    
    

    
    
    

    
    

    
    
    

    
    
    <script>
      hljs.initHighlightingOnLoad();

    </script>
    

    

    
    

    

    
    

    
    

    
    
    
    
    
    
    
    
    
    
    
    
    <script src="/js/academic.min.d4cda92104b2116795ec97f53b402511.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
