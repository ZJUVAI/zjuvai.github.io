[{"authors":["minfengzhu"],"categories":null,"content":"","date":1603826120,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1603826120,"objectID":"d2e2d2887bb13006210f344c3d2b4856","permalink":"/authors/minfengzhu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/minfengzhu/","section":"authors","summary":"","tags":null,"title":"Minfeng Zhu","type":"authors"},{"authors":["jiachengpan"],"categories":null,"content":"","date":1603670400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1603670400,"objectID":"4fb9f5907ebada51e66a95964ef449db","permalink":"/authors/jiachengpan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jiachengpan/","section":"authors","summary":"","tags":null,"title":"Jiacheng Pan","type":"authors"},{"authors":["zhaosonghuang"],"categories":null,"content":"","date":1571967020,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1571967020,"objectID":"3733917af20bc801c57fb5dfabff0a07","permalink":"/authors/zhaosonghuang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zhaosonghuang/","section":"authors","summary":"","tags":null,"title":"Zhaosong Huang","type":"authors"},{"authors":["yatingwei"],"categories":null,"content":"","date":1571735420,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1571735420,"objectID":"7f6fca97fe9dc0fb109be329aebe5385","permalink":"/authors/yatingwei/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yatingwei/","section":"authors","summary":"","tags":null,"title":"Yating Wei","type":"authors"},{"authors":["xvmengwang"],"categories":null,"content":"","date":1540431380,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1540431380,"objectID":"6b03c0e034b79430d4a1272d1d959f22","permalink":"/authors/xumengwang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xumengwang/","section":"authors","summary":"","tags":null,"title":"Xumeng Wang","type":"authors"},{"authors":["dongminghan"],"categories":null,"content":"","date":1540352600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1540352600,"objectID":"09d65d06c3a9fe08d385587c9e6e87c5","permalink":"/authors/dongminghan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/dongminghan/","section":"authors","summary":"","tags":null,"title":"Dongming Han","type":"authors"},{"authors":["feiranwu"],"categories":null,"content":"","date":1524210362,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1524210362,"objectID":"b8defc4371af614012660e497941d38f","permalink":"/authors/feiranwu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/feiranwu/","section":"authors","summary":"","tags":null,"title":"Feiran Wu","type":"authors"},{"authors":["admin"],"categories":null,"content":"The Visual Analytics and Intelligence Group of the State Key lab of CAD\u0026amp;CG, Zhejiang University was established in September 2008. The academic leader is Professor Chen Wei (Weibo: 浙大可视分析小组).\nOur group focuses on the research of VISUALIZATION and VISUAL ANALYTICS. The research direction mainly includes: data science, visualization of complex data, basic theory and method of visual analysis, and domain-oriented visual analysis prototype system. Our group members have published more than 100 papers on IEEE VIS Conference and IEEE/ACM journals, including the first three papers published by the Chinese mainland at this conference (2004, 2009). The team works extensively with universities and research institutions at home and abroad, including Purdue University, Hong Kong University of Science and Technology, University of California, Davis, North Carolina, Mississippi State University, Bosch North American Institute, Microsoft Research Asia, National Meteorological Administration. , Alibaba Group and so on. The world\u0026rsquo;s first non-photorealistic 3D GPS navigation system, developed by our group\u0026rsquo;s five members at the Bosch North American Institute, has entered the global automotive market. Our group has developed a data visualization component library, DataV, together with the data product division of Ali Group. It has now been widely used within the Ali Group. The global scale 3D numerical atmospheric visual analysis system developed by our group has been well received by the National Meteorological Administration.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"The Visual Analytics and Intelligence Group of the State Key lab of CAD\u0026amp;CG, Zhejiang University was established in September 2008. The academic leader is Professor Chen Wei (Weibo: 浙大可视分析小组).\nOur group focuses on the research of VISUALIZATION and VISUAL ANALYTICS. The research direction mainly includes: data science, visualization of complex data, basic theory and method of visual analysis, and domain-oriented visual analysis prototype system. Our group members have published more than 100 papers on IEEE VIS Conference and IEEE/ACM journals, including the first three papers published by the Chinese mainland at this conference (2004, 2009).","tags":null,"title":"ZJU VAI","type":"authors"},{"authors":["baichengwang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"1313e3cc31acf3a9c6e8ecf374449a37","permalink":"/authors/baichengwang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/baichengwang/","section":"authors","summary":"","tags":null,"title":"Baicheng Wang","type":"authors"},{"authors":["biaozhu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e92e23265954bb402633cc4fca47b6c7","permalink":"/authors/biaozhu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/biaozhu/","section":"authors","summary":"","tags":null,"title":"Biao Zhu","type":"authors"},{"authors":["binghuiyan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"332b65ba672137e4e30a8519ad0630a1","permalink":"/authors/binghuiyan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/binghuiyan/","section":"authors","summary":"","tags":null,"title":"Binghui Yan","type":"authors"},{"authors":["bingrulin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3a2ae9400bfd4148d0695ace0ec9b9d0","permalink":"/authors/bingrulin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/bingrulin/","section":"authors","summary":"","tags":null,"title":"Bingru Lin","type":"authors"},{"authors":["binzhou"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"63dabb1cfad3c195ae8c54b9063e898f","permalink":"/authors/binzhou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/binzhou/","section":"authors","summary":"","tags":null,"title":"Bin Zhou","type":"authors"},{"authors":["bopan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"beee14229f6042b2854a1e828820b5e1","permalink":"/authors/bopan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/bopan/","section":"authors","summary":"","tags":null,"title":"Bo Pan","type":"authors"},{"authors":["chaokaiwen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"688ec6fdc0310d45cd8aee8ee3e4cb5a","permalink":"/authors/chaokaiwen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/chaokaiwen/","section":"authors","summary":"","tags":null,"title":"Chaokai Wen","type":"authors"},{"authors":["congxie"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"511f357aa3e547021f51ef30717551f3","permalink":"/authors/congxie/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/congxie/","section":"authors","summary":"","tags":null,"title":"Cong Xie","type":"authors"},{"authors":["dizhang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e67572d7f4d3697d78088461e8e47ebe","permalink":"/authors/dizhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/dizhang/","section":"authors","summary":"","tags":null,"title":"Di Zhang","type":"authors"},{"authors":["erqingzhang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"46225d6e6f381fddc357d41654e4b4d9","permalink":"/authors/erqingzhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/erqingzhang/","section":"authors","summary":"","tags":null,"title":"Erqing Zhang","type":"authors"},{"authors":["fangzhouguo"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a0315413f270f825b2780dba83a67903","permalink":"/authors/fangzhouguo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/fangzhouguo/","section":"authors","summary":"","tags":null,"title":"Fangzhou Guo","type":"authors"},{"authors":["fanyan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f214ff4a10bf6ecb96d09346184e3ef6","permalink":"/authors/fanyan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/fanyan/","section":"authors","summary":"","tags":null,"title":"Fan Yan","type":"authors"},{"authors":["guangyuchen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"10a25c87b69bfaec2b3313c852fac72c","permalink":"/authors/guangyuchen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/guangyuchen/","section":"authors","summary":"","tags":null,"title":"Guangyu Chen","type":"authors"},{"authors":["guizhenwang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5e5db27e9da0ce449fdc31733d1e3abc","permalink":"/authors/guizhenwang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/guizhenwang/","section":"authors","summary":"","tags":null,"title":"Guizhen Wang","type":"authors"},{"authors":["haidongchen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"1ec39a231beee315ede98cbe388d47d5","permalink":"/authors/haidongchen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/haidongchen/","section":"authors","summary":"","tags":null,"title":"Haidong Chen","type":"authors"},{"authors":["haiyizhou"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9548bd15c1da905a89378bb6422901a0","permalink":"/authors/haiyizhou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/haiyizhou/","section":"authors","summary":"","tags":null,"title":"Haiyi Zhou","type":"authors"},{"authors":["hangzhu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"10183f697a9a3a93486d156f5c9113c0","permalink":"/authors/hangzhu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/hangzhu/","section":"authors","summary":"","tags":null,"title":"Hang Zhu","type":"authors"},{"authors":["haonanliu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d30e87c5a8af87c190d8d1ea6143f838","permalink":"/authors/haonanliu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/haonanliu/","section":"authors","summary":"","tags":null,"title":"Haonan Liu","type":"authors"},{"authors":["haoyutian"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"002ea48c87e249ca258853cc288d5f6f","permalink":"/authors/haoyutian/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/haoyutian/","section":"authors","summary":"","tags":null,"title":"Haoyu Tian","type":"authors"},{"authors":["haozhefeng"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e42d6a3136ba277bd69c8cb130f9e331","permalink":"/authors/haozhefeng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/haozhefeng/","section":"authors","summary":"","tags":null,"title":"Haozhe Feng","type":"authors"},{"authors":["hongboliu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"063d7daa7d67fc8ee85b2482907616be","permalink":"/authors/hongboliu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/hongboliu/","section":"authors","summary":"","tags":null,"title":"Hongbo Liu","type":"authors"},{"authors":["honghuimei"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"58c4d52fdd159c66b67b8953d7b59569","permalink":"/authors/honghuimei/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/honghuimei/","section":"authors","summary":"","tags":null,"title":"Honghui Mei","type":"authors"},{"authors":["hongkunpan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"bbb69464ab8f8132225671e7e1fce853","permalink":"/authors/hongkunpan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/hongkunpan/","section":"authors","summary":"","tags":null,"title":"Hongkun Pan","type":"authors"},{"authors":["huanliangwang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ebb15fb168bb565c132b7a079bbab06c","permalink":"/authors/huanliangwang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/huanliangwang/","section":"authors","summary":"","tags":null,"title":"Huanliang Wang","type":"authors"},{"authors":["huihuaguan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f38c08521b6d2d54a320ccd4c7157650","permalink":"/authors/huihuaguan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/huihuaguan/","section":"authors","summary":"","tags":null,"title":"Huihua Guan","type":"authors"},{"authors":["huiye"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"21676729a800269358d7bdda88ad3c95","permalink":"/authors/huiye/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/huiye/","section":"authors","summary":"","tags":null,"title":"Hui Ye","type":"authors"},{"authors":["jianghuatan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"dddc95ea0bce3a0143b41bc272d0bcfa","permalink":"/authors/jianghuatan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jianghuatan/","section":"authors","summary":"","tags":null,"title":"Jianghua Tan","type":"authors"},{"authors":["jianshen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"548de1a7d2b99bd824fca91abfe91ce4","permalink":"/authors/jianshen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jianshen/","section":"authors","summary":"","tags":null,"title":"Jian Shen","type":"authors"},{"authors":["jianweizhang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b3c918c34f1f2f1ec813f07c2d25fc85","permalink":"/authors/jianweizhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jianweizhang/","section":"authors","summary":"","tags":null,"title":"Jianwei Zhang","type":"authors"},{"authors":["jiashunsun"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"173afa7a6fb905ad48b9eaaf4d59b111","permalink":"/authors/jiashunsun/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jiashunsun/","section":"authors","summary":"","tags":null,"title":"Jiashun Sun","type":"authors"},{"authors":["jiayinglu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2b5e8ea8aa4af46066bd41cddebcf029","permalink":"/authors/jiayinglu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jiayinglu/","section":"authors","summary":"","tags":null,"title":"Jiaying Lu","type":"authors"},{"authors":["jiazhixia"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"701147a8205ac3d89f3a3af91cd8906d","permalink":"/authors/jiazhixia/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jiazhixia/","section":"authors","summary":"","tags":null,"title":"Jiazhi Xia","type":"authors"},{"authors":["jiehuizhou"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ecf94669d677925eacdd07fb6e2eecc6","permalink":"/authors/jiehuizhou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jiehuizhou/","section":"authors","summary":"","tags":null,"title":"Jiehui Zhou","type":"authors"},{"authors":["jiewang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"81f2479fbce3bbda2fbae0889ec3bdfc","permalink":"/authors/jiewang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jiewang/","section":"authors","summary":"","tags":null,"title":"Jie Wang","type":"authors"},{"authors":["jieyichen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b64b982df96c06a2625a4eec1333af71","permalink":"/authors/jieyichen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jieyichen/","section":"authors","summary":"","tags":null,"title":"Jieyi Chen","type":"authors"},{"authors":["jinglixu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"4ce8d75fde5d06b78f0badc0477bed6c","permalink":"/authors/jinglixu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jinglixu/","section":"authors","summary":"","tags":null,"title":"Jingli Xu","type":"authors"},{"authors":["jingxia"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5b34a2bc6b2e6ef52a26e937f64bcdf4","permalink":"/authors/jingxia/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jingxia/","section":"authors","summary":"","tags":null,"title":"Jing Xia","type":"authors"},{"authors":["jinzewu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"18dd2929b50f2844cfbf45beda10260e","permalink":"/authors/jinzewu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jinzewu/","section":"authors","summary":"","tags":null,"title":"Jinze Wu","type":"authors"},{"authors":["junhualu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a064b7e3b2d2ced15b4acef508a1049e","permalink":"/authors/junhualu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/junhualu/","section":"authors","summary":"","tags":null,"title":"Junhua Lu","type":"authors"},{"authors":["junminglu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"38dcafdb2cb4bc6dd20d9c0a8c88d14e","permalink":"/authors/junminglu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/junminglu/","section":"authors","summary":"","tags":null,"title":"Junming Lu","type":"authors"},{"authors":["junyuLu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"1cf9a9b4de9009fcb69ded6343540cd4","permalink":"/authors/junyulu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/junyulu/","section":"authors","summary":"","tags":null,"title":"Junyu Lu","type":"authors"},{"authors":["kejieyu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"618564ca10a1f5ed7a789b6f96efb2fa","permalink":"/authors/kejieyu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/kejieyu/","section":"authors","summary":"","tags":null,"title":"Kejie Yu","type":"authors"},{"authors":["ketianmao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"747a338a23bf8a81398284ff4af42f6c","permalink":"/authors/ketianmao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ketianmao/","section":"authors","summary":"","tags":null,"title":"Ketian Mao","type":"authors"},{"authors":["leilv"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"176a3a7866c1e2208c79810d3ea5e47c","permalink":"/authors/leilv/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/leilv/","section":"authors","summary":"","tags":null,"title":"Lei Lv","type":"authors"},{"authors":["liangjunliu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9e1aa6b0897bb8612d697c3cea7a483a","permalink":"/authors/liangjunliu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/liangjunliu/","section":"authors","summary":"","tags":null,"title":"Liangjun Liu","type":"authors"},{"authors":["lingzhang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"73798baa9c2efed8dc2799da7cf924ad","permalink":"/authors/lingzhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/lingzhang/","section":"authors","summary":"","tags":null,"title":"Ling Zhang","type":"authors"},{"authors":["linhaomeng"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"47178a400c59f544be2df10ed1936483","permalink":"/authors/linhaomeng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/linhaomeng/","section":"authors","summary":"","tags":null,"title":"Linhao Meng","type":"authors"},{"authors":["linzhu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e62e6c889920a8762d6481bdccf221c8","permalink":"/authors/linzhu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/linzhu/","section":"authors","summary":"","tags":null,"title":"Lin Zhu","type":"authors"},{"authors":["liwenlin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e40f4f87ed759a06e9b94a2475c2c57a","permalink":"/authors/liwenlin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/liwenlin/","section":"authors","summary":"","tags":null,"title":"Liwen Lin","type":"authors"},{"authors":["lizheng"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"4bb9e7fdff270d1ead4eebe4f15cd6d2","permalink":"/authors/lizheng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/lizheng/","section":"authors","summary":"","tags":null,"title":"Li Zheng","type":"authors"},{"authors":["lonapalawongsupaporn"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6430ec5fa919d99a92b083d106ec81db","permalink":"/authors/lonapalawongsupaporn/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/lonapalawongsupaporn/","section":"authors","summary":"","tags":null,"title":"Lonapalawong Supaporn","type":"authors"},{"authors":["luoxuanweng"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"eb304a91533a1f871a5838ec94d29051","permalink":"/authors/luoxuanweng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/luoxuanweng/","section":"authors","summary":"","tags":null,"title":"Luoxuan Weng","type":"authors"},{"authors":["miaojunyao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"578ef7122ec9ebe17a324b511cc0ade9","permalink":"/authors/miaojunyao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/miaojunyao/","section":"authors","summary":"","tags":null,"title":"Miaojun Yao","type":"authors"},{"authors":["minglin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7135bc5611cf18d390d867c5831d12d6","permalink":"/authors/minglin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/minglin/","section":"authors","summary":"","tags":null,"title":"Ming Lin","type":"authors"},{"authors":["noptanitchotisarn"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6a7eec2abe6ed13991d82a70ef34b879","permalink":"/authors/noptanitchotisarn/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/noptanitchotisarn/","section":"authors","summary":"","tags":null,"title":"Noptanit Chotisarn","type":"authors"},{"authors":["qinxianliu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6c53ad8a6801919ca206907b21aa6dd0","permalink":"/authors/qinxianliu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/qinxianliu/","section":"authors","summary":"","tags":null,"title":"Qinxian Liu","type":"authors"},{"authors":["qitongyan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"823b48f18325dde3295756137c2f9313","permalink":"/authors/qitongyan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/qitongyan/","section":"authors","summary":"","tags":null,"title":"Qitong Yan","type":"authors"},{"authors":["qiwang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6e4434de440cfad22549897f7d20a6e3","permalink":"/authors/qiwang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/qiwang/","section":"authors","summary":"","tags":null,"title":"Qi Wang","type":"authors"},{"authors":["qiuxuyao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"c75bbc53f04019a4ff1d0527acf9b436","permalink":"/authors/qixuyao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/qixuyao/","section":"authors","summary":"","tags":null,"title":"Qixu Yao","type":"authors"},{"authors":["rongchenzhu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ccc12a2e7e41c903d30c55dd1c8b8e5a","permalink":"/authors/rongchenzhu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/rongchenzhu/","section":"authors","summary":"","tags":null,"title":"Rongchen Zhu","type":"authors"},{"authors":["ruizhepan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"498f9a126797622e3843382368c1b3aa","permalink":"/authors/ruizhepan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ruizhepan/","section":"authors","summary":"","tags":null,"title":"Ruizhe Pan","type":"authors"},{"authors":["rushengpan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6d25cf561fd51b42f5cbd8309adeaa7e","permalink":"/authors/rushengpan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/rushengpan/","section":"authors","summary":"","tags":null,"title":"Rusheng Pan","type":"authors"},{"authors":["shenghuihu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2f2c8d7a26761fce09ece8e4e7c4be6e","permalink":"/authors/shenghuihu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/shenghuihu/","section":"authors","summary":"","tags":null,"title":"Shenghui Hu","type":"authors"},{"authors":["shengjiegao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"67a79c2b02345bc1e435708934246d65","permalink":"/authors/shengjiegao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/shengjiegao/","section":"authors","summary":"","tags":null,"title":"Shengjie Gao","type":"authors"},{"authors":["shiliu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2877b09adad4677f30a5f6be2810ba72","permalink":"/authors/shiliu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/shiliu/","section":"authors","summary":"","tags":null,"title":"Shi Liu","type":"authors"},{"authors":["shiweicao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"170d2bf94d91bdc2f86c6b7005c27fff","permalink":"/authors/shiweicao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/shiweicao/","section":"authors","summary":"","tags":null,"title":"Shiwei Cao","type":"authors"},{"authors":["shuangye"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f8f19cc9794ce84d8eba1ee738dae58a","permalink":"/authors/shuangye/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/shuangye/","section":"authors","summary":"","tags":null,"title":"Shuang Ye","type":"authors"},{"authors":["shuyuezhou"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a07ca3bd20c7f1c5f1254e02b72f184e","permalink":"/authors/shuyuezhou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/shuyuezhou/","section":"authors","summary":"","tags":null,"title":"Shuyue Zhou","type":"authors"},{"authors":["sijiawang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"18ac02cd630fa4169c183f17e2c2cc97","permalink":"/authors/sijiawang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/sijiawang/","section":"authors","summary":"","tags":null,"title":"Sijia Wang","type":"authors"},{"authors":["sikanzhu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6a43798cb4e41d839a9b08725bf1bbcf","permalink":"/authors/sikanzhu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/sikanzhu/","section":"authors","summary":"","tags":null,"title":"Sikan Zhu","type":"authors"},{"authors":["tianshengzhang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ae5d3c06faee8f2a9ec871a762213cf0","permalink":"/authors/tianshengzhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/tianshengzhang/","section":"authors","summary":"","tags":null,"title":"Tiansheng Zhang","type":"authors"},{"authors":["tianyezhang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e9e1cdffe6711fa9a43b0864dec396c6","permalink":"/authors/tianyezhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/tianyezhang/","section":"authors","summary":"","tags":null,"title":"Tianye Zhang","type":"authors"},{"authors":["tianyilao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9f8dcb22e7f37be548cf2fd8763517f7","permalink":"/authors/tianyilao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/tianyilao/","section":"authors","summary":"","tags":null,"title":"Tianyi Lao","type":"authors"},{"authors":["tongxu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"67aa67a0704d24dd3347619b51ffc9db","permalink":"/authors/tongxu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/tongxu/","section":"authors","summary":"","tags":null,"title":"Tong Xu","type":"authors"},{"authors":["wanqihu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"932447e81dfa33015524b8d0f96ffff3","permalink":"/authors/wanqihu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/wanqihu/","section":"authors","summary":"","tags":null,"title":"Wanqi Hu","type":"authors"},{"authors":["weichen"],"categories":null,"content":"Wei Chen is a professor in State Key Lab of CAD\u0026amp;CG at Zhejiang University, P.R.China. His current research interests include visualization, visual analytics and bio-medical image computing. From 1992 to 2002, He spent 8 years at Zhejiang University, where he got his bachelor and Ph.D degree. From 2000 to 2002, he was a visiting Ph.D student in Fraunhofer Institute for Graphics, Darmstadt, Germany. Thereafter he got his Ph.D and joined Zhejiang University. From July. 2006 to Sep. 2008, Dr. Wei Chen was a visiting scholar at Purdue University, working in PURPL with Prof.David S. Ebert. In December 2009, He was promoted as a full professor of Zhejiang University. He has performed research in visualization and visual analysis and published more than 100 IEEE/ACM Transactions and IEEE VIS papers. His chinese books on visualization are the unique books on visualization in China. He actively served in many leading conferences and journals, like IEEE PacificVIS steering committee, ChinaVIS steering committee, paper co-chairs of IEEE PacificVIS, IEEE LDAV and ACM SIGGRAPH Asia VisSym. He is an associate editor of ACM TIST, IEEE CG\u0026amp;A, FCS and JOV.\nHere is the homepage of visual analytics group of State Key Lab of CAD\u0026amp;CG, Zhejiang University.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"611789f74777bc3ea79ff9520e093a52","permalink":"/authors/weichen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/weichen/","section":"authors","summary":"Wei Chen is a professor in State Key Lab of CAD\u0026amp;CG at Zhejiang University, P.R.China. His current research interests include visualization, visual analytics and bio-medical image computing. From 1992 to 2002, He spent 8 years at Zhejiang University, where he got his bachelor and Ph.D degree. From 2000 to 2002, he was a visiting Ph.D student in Fraunhofer Institute for Graphics, Darmstadt, Germany. Thereafter he got his Ph.D and joined Zhejiang University.","tags":null,"title":"Wei Chen","type":"authors"},{"authors":["weixiaxu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f5636b7d8935f736a3e99f5992265e74","permalink":"/authors/weixiaxu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/weixiaxu/","section":"authors","summary":"","tags":null,"title":"Weixia Xu","type":"authors"},{"authors":["weizhang1"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"94b33ef33d0d224ffb1c4ca364f74c3f","permalink":"/authors/weizhang1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/weizhang1/","section":"authors","summary":"","tags":null,"title":"Wei Zhang","type":"authors"},{"authors":["weizhang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"05567cddfcd361074feba76c2de8fa34","permalink":"/authors/weizhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/weizhang/","section":"authors","summary":"","tags":null,"title":"Wei Zhang","type":"authors"},{"authors":["wenjielu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"fff1a80c32ac66eee12b7369714a3555","permalink":"/authors/wenjielu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/wenjielu/","section":"authors","summary":"","tags":null,"title":"Wenjie Lu","type":"authors"},{"authors":["wenlongchen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"12a7952e4615b8536f9124599624ee7b","permalink":"/authors/wenlongchen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/wenlongchen/","section":"authors","summary":"","tags":null,"title":"Wenlong Chen","type":"authors"},{"authors":["xiaodongzhao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"dbc5ba7e8a284577aa94508220ccc51a","permalink":"/authors/xiaodongzhao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xiaodongzhao/","section":"authors","summary":"","tags":null,"title":"Xiaodong Zhao","type":"authors"},{"authors":["xiaohongma"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"02b9a3ac6efa0aa1d6da6f535b244118","permalink":"/authors/xiaohongma/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xiaohongma/","section":"authors","summary":"","tags":null,"title":"Xiaohong Ma","type":"authors"},{"authors":["xiaoyuyang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a3d66f74246da177308cdec53d0bc728","permalink":"/authors/xiaoyuyang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xiaoyuyang/","section":"authors","summary":"","tags":null,"title":"Xiaoyu Yang","type":"authors"},{"authors":["xingxu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5a692e5cc99d45d2fc9380c9865818ea","permalink":"/authors/xingxu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xingxu/","section":"authors","summary":"","tags":null,"title":"Xing Xu","type":"authors"},{"authors":["xinxinhuang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9338c969bf89e7be92faa75c8a73f8d8","permalink":"/authors/xinxinhuang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xinxinhuang/","section":"authors","summary":"","tags":null,"title":"Xinxin Huang","type":"authors"},{"authors":["xinzhao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"aa9709c796c5bdc99ccf45a239d659cd","permalink":"/authors/xinzhao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xinzhao/","section":"authors","summary":"","tags":null,"title":"Xin Zhao","type":"authors"},{"authors":["xiuqihuang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"23e2922aaf6e5517baae171afd841fa6","permalink":"/authors/xiuqihuang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xiuqihuang/","section":"authors","summary":"","tags":null,"title":"Xiuqi Huang","type":"authors"},{"authors":["yichaowang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"97c3335e53dd5045cf8375077f3441ff","permalink":"/authors/yichaowang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yichaowang/","section":"authors","summary":"","tags":null,"title":"Yichao Wang","type":"authors"},{"authors":["yifangren"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"58f18f837c1837256df908c8b673fa4d","permalink":"/authors/yifangren/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yifangren/","section":"authors","summary":"","tags":null,"title":"Yifang Ren","type":"authors"},{"authors":["yihanliu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f783e19a6d4625eb2f0893aeeaed71fe","permalink":"/authors/yihanliu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yihanliu/","section":"authors","summary":"","tags":null,"title":"Yihan Liu","type":"authors"},{"authors":["yijingliu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7110456b92de4eeab4224e4188560b5f","permalink":"/authors/yijingliu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yijingliu/","section":"authors","summary":"","tags":null,"title":"Yijing Liu","type":"authors"},{"authors":["yingchaojiefeng"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5f90602427a5323dd3c873fc6698c940","permalink":"/authors/yingchaojiefeng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yingchaojiefeng/","section":"authors","summary":"","tags":null,"title":"Yingchaojie Feng","type":"authors"},{"authors":["yinghaotang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e05bd492de2290a74ebc5c63e2ca5aba","permalink":"/authors/yinghaotang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yinghaotang/","section":"authors","summary":"","tags":null,"title":"Yinghao Tang","type":"authors"},{"authors":["yingxu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a3d45e8ffeb1f08addba4c8c9545f41b","permalink":"/authors/yingxu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yingxu/","section":"authors","summary":"","tags":null,"title":"Ying Xu","type":"authors"},{"authors":["yiren"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"da2dc090db9311db597dd86c62f27383","permalink":"/authors/yiren/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yiren/","section":"authors","summary":"","tags":null,"title":"Yi Ren","type":"authors"},{"authors":["yitianchen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d3da587ab73250d5097b0576f44c2691","permalink":"/authors/yitianchen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yitianchen/","section":"authors","summary":"","tags":null,"title":"Yitian Chen","type":"authors"},{"authors":["yixiaofu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"609b9e13fb9a3da30d7fa86a2e2e2a64","permalink":"/authors/yixiaofu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yixiaofu/","section":"authors","summary":"","tags":null,"title":"Yixiao Fu","type":"authors"},{"authors":["yiyang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a16e3f324e2e275d055143d372890161","permalink":"/authors/yiyang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yiyang/","section":"authors","summary":"","tags":null,"title":"Yi Yang","type":"authors"},{"authors":["yiyaowang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8b948d75f1b1730f52fdb348660f794e","permalink":"/authors/yiyaowang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yiyaowang/","section":"authors","summary":"","tags":null,"title":"Yiyao Wang","type":"authors"},{"authors":["yongdai"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"4f0ea0db8039d21965ed04c7623396f9","permalink":"/authors/yongdai/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yongdai/","section":"authors","summary":"","tags":null,"title":"Yong Dai","type":"authors"},{"authors":["youchenggong"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"88c5cbdae50149d621ac4f6d5b2ad4f6","permalink":"/authors/youchenggong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/youchenggong/","section":"authors","summary":"","tags":null,"title":"Youcheng Gong","type":"authors"},{"authors":["yuanzhehu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e5db2f36fde56f28f6f63167d31fb711","permalink":"/authors/yuanzhehu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yuanzhehu/","section":"authors","summary":"","tags":null,"title":"Yuanzhe Hu","type":"authors"},{"authors":["yuefanzhou"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"abdb93fc39589867c8bab3cf11c1232c","permalink":"/authors/yuefanzhou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yuefanzhou/","section":"authors","summary":"","tags":null,"title":"Yuefan Zhou","type":"authors"},{"authors":["yuhonglu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3d5dac4c96acde85d5928aa14f727807","permalink":"/authors/yuhonglu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yuhonglu/","section":"authors","summary":"","tags":null,"title":"Yuhong Lu","type":"authors"},{"authors":["yuhuigu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"525f85d17df210782f32184018c43cd8","permalink":"/authors/yuhuigu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yuhuigu/","section":"authors","summary":"","tags":null,"title":"Yuhui Gu","type":"authors"},{"authors":["yuhuizhang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a08b195a6f791218e268f6d1f54cd788","permalink":"/authors/yuhuizhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yuhuizhang/","section":"authors","summary":"","tags":null,"title":"Yuhui Zhang","type":"authors"},{"authors":["yumenghou"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b7c153824af0c8dd7692508d5ad947bd","permalink":"/authors/yumenghou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yumenghou/","section":"authors","summary":"","tags":null,"title":"Yumeng Hou","type":"authors"},{"authors":["yuweiwu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"814e70ad5ecbd563827bb065ee3e824f","permalink":"/authors/yuweiwu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yuweiwu/","section":"authors","summary":"","tags":null,"title":"Yuwei Wu","type":"authors"},{"authors":["yuxinma"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"4fc076713713879f65404b3f0466ec46","permalink":"/authors/yuxinma/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yuxinma/","section":"authors","summary":"","tags":null,"title":"Yuxin Ma","type":"authors"},{"authors":["yuxuanhou"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"359b2cdc705132c678015f91b08c756b","permalink":"/authors/yuxuanhou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yuxuanhou/","section":"authors","summary":"","tags":null,"title":"Yuxuan Hou","type":"authors"},{"authors":["zechenwang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f0fb23dab9c54499ae9b6af6d8e35702","permalink":"/authors/zechenwang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zechenwang/","section":"authors","summary":"","tags":null,"title":"Zechen Wang","type":"authors"},{"authors":["zexianchen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"72b1c74afd05b0bbbe06e0b2c86b7c17","permalink":"/authors/zexianchen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zexianchen/","section":"authors","summary":"","tags":null,"title":"Zexian Chen","type":"authors"},{"authors":["zhaoruiyang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"4f743995b480a7299942f4ec8d4dd085","permalink":"/authors/zhaoruiyang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zhaoruiyang/","section":"authors","summary":"","tags":null,"title":"Zhaorui Yang","type":"authors"},{"authors":["zhenwen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"57bf2a093e49d69402d447dfadbf046a","permalink":"/authors/zhenwen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zhenwen/","section":"authors","summary":"","tags":null,"title":"Zhen Wen","type":"authors"},{"authors":["zhezhao"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"79c59e66f2a141087e56ef47c2636661","permalink":"/authors/zhezhao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zhezhao/","section":"authors","summary":"","tags":null,"title":"Zhe Zhao","type":"authors"},{"authors":["zhichengyan"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e620a9f11281441d7f58c52f1e59a582","permalink":"/authors/zhichengyan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zhichengyan/","section":"authors","summary":"","tags":null,"title":"Zhicheng Yan","type":"authors"},{"authors":["zhiqiliu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a3349d4d8943d12c85cbded143581e52","permalink":"/authors/zhiqiliu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zhiqiliu/","section":"authors","summary":"","tags":null,"title":"Zhiqi Liu","type":"authors"},{"authors":["zhiyongwang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2c51f279f4b3732c7164c6424abed2bc","permalink":"/authors/zhiyongwang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zhiyongwang/","section":"authors","summary":"","tags":null,"title":"Zhiyong Wang","type":"authors"},{"authors":["zhiyuding"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b81096a67cbe47badd143118963d9a40","permalink":"/authors/zhiyuding/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zhiyuding/","section":"authors","summary":"","tags":null,"title":"Zhiyu Ding","type":"authors"},{"authors":["zhizhangchen"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8efadec5bfaa3b317ea9d02686825662","permalink":"/authors/zhizhangchen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zhizhangchen/","section":"authors","summary":"","tags":null,"title":"Zhizhang Chen","type":"authors"},{"authors":["zhonghaoqian"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8de4e0ece238d2b2ade7a24d298e8a1e","permalink":"/authors/zhonghaoqian/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zhonghaoqian/","section":"authors","summary":"","tags":null,"title":"Zhonghao Qian","type":"authors"},{"authors":["zhongweiwang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"134ad86e7c516aa9c6313ef322d4694d","permalink":"/authors/zhongweiwang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zhongweiwang/","section":"authors","summary":"","tags":null,"title":"Zhongwei Wang","type":"authors"},{"authors":["zhuoyundu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"c03d3f94b922e7e197fb06b028b0c7e9","permalink":"/authors/zhuoyundu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zhuoyundu/","section":"authors","summary":"","tags":null,"title":"Zhuoyun Du","type":"authors"},{"authors":["zihanzhou"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"1fc3a620590a16c2ad2094a714519f20","permalink":"/authors/zihanzhou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zihanzhou/","section":"authors","summary":"","tags":null,"title":"Zihan Zhou","type":"authors"},{"authors":["ziliangwu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e277abbb99dc40ff3207c4c636772965","permalink":"/authors/ziliangwu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ziliangwu/","section":"authors","summary":"","tags":null,"title":"Ziliang Wu","type":"authors"},{"authors":["zongzhuangli"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a263db619c550cce6a6c9af0d84681c7","permalink":"/authors/zongzhuangli/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zongzhuangli/","section":"authors","summary":"","tags":null,"title":"Zongzhuang Li","type":"authors"},{"authors":["Luoxuan Weng","Yinghao Tang","Yingchaojie Feng","Zhuo Chang","Ruiqin Chen","Haozhe Feng","Chen Hou","Danqing Huang","Yang Li","Huaming Rao","Haonan Wang","Canshi Wei","Xiaofeng Yang","Yuhui Zhang","Yifeng Zheng","Xiuqi Huang","Minfeng Zhu","Yuxin Ma","Bin Cui","Peng Chen","Wei Chen"],"categories":[],"content":"","date":1748822400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748822400,"objectID":"5540ac7f9b1157a71297fe5f22602ca3","permalink":"/publications/llm-powered-enterprise-intelligence/","publishdate":"2025-06-02T00:00:00Z","relpermalink":"/publications/llm-powered-enterprise-intelligence/","section":"publications","summary":"In this paper, we introduce the Enterprise Intelligence Pipeline, an automated system designed to extract, verify, and match primary business data using a combination of AI-powered tools and structured rule-based logic. The pipeline leverages natural language processing and a Company Identity Matcher module to enrich minimal input queries with accurate and reliable information. Its modular architecture, orchestrated via Amazon ECS, ensures scalability, traceability, and high data quality across enterprise workflows. The system significantly reduces manual effort, improves data consistency, and integrates seamlessly into broader decision-making platforms. Additionally, it supports real-time data validation and feedback mechanisms, enabling continuous enhancement of data accuracy. With a plug-and-play framework, the pipeline is easily customizable for industry-specific applications. By integrating deterministic rules with probabilistic AI models, it offers a balanced solution that combines precision with adaptability.   Keywords— Primary Intelligence, Data Verification, NLP, AI Pipeline, Company Identity Matcher Matching, Microservices, Amazon ECS, Data Enrichment.","tags":["large language models"],"title":"DataLab: A Unified Platform for LLM-Powered Business Intelligence","type":"publications"},{"authors":["Luoxuan Weng","Xingbo Wang","Junyu Lu","Yingchaojie Feng","Yihan Liu","Haozhe Feng","Danqing Huang","Wei Chen"],"categories":[],"content":"","date":1746489600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1746489600,"objectID":"b36b09df5ee3bc5cedbd2cb9313fb590","permalink":"/publications/insightlens-augmenting-llm-powered-data-analysis-with-interactive-insight/","publishdate":"2025-05-06T00:00:00Z","relpermalink":"/publications/insightlens-augmenting-llm-powered-data-analysis-with-interactive-insight/","section":"publications","summary":"The proliferation of large language models (LLMs) has revolutionized the capabilities of natural language interfaces (NLIs) for data analysis. LLMs can perform multi-step and complex reasoning to generate data insights based on users’ analytic intents. However, these insights often entangle with an abundance of contexts in analytic conversations such as code, visualizations, and natural language explanations. This hinders efficient recording, organization, and navigation of insights within the current chat-based LLM interfaces. In this paper, we first conduct a formative study with eight data analysts to understand their general workflow and pain points of insight management during LLM-powered data analysis. Accordingly, we introduce InsightLens, an interactive system to overcome such challenges. Built upon an LLM-agent-based framework that automates insight recording and organization along with the analysis process, InsightLens visualizes the complex conversational contexts from multiple aspects to facilitate insight navigation. A user study with twelve data analysts demonstrates the effectiveness of InsightLens, showing that it significantly reduces users’ manual and cognitive effort without disrupting their conversational data analysis workflow, leading to a more efficient analysis experience.","tags":["large language models"],"title":"InsightLens: Augmenting LLM-Powered Data Analysis With Interactive Insight Management and Navigation","type":"publications"},{"authors":["Yingchaojie Feng","Zhizhang Chen","Zhining Kang","Sijia Wang","Haoyu Tian","Wei Zhang","Minfeng Zhu","Wei Chen"],"categories":[],"content":"","date":1735689600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1735689600,"objectID":"a6273826115e8c93d51d2e9a61c55957","permalink":"/publications/jailbreaklens-visual-analysis-of-jailbreak-attacks-against-large/","publishdate":"2025-01-01T00:00:00Z","relpermalink":"/publications/jailbreaklens-visual-analysis-of-jailbreak-attacks-against-large/","section":"publications","summary":"The proliferation of large language models (LLMs) has underscored concerns regarding their security vulnerabilities, notably against jailbreak attacks, where adversaries design jailbreak prompts to circumvent safety mechanisms for potential misuse. Addressing these concerns necessitates a comprehensive analysis of jailbreak prompts to evaluate LLMs' defensive capabilities and identify potential weaknesses. However, the complexity of evaluating jailbreak performance and understanding prompt characteristics makes this analysis laborious. We collaborate with domain experts to characterize problems and propose an LLM-assisted framework to streamline the analysis process. It provides automatic jailbreak assessment to facilitate performance evaluation and support analysis of components and keywords in prompts. Based on the framework, we design JailbreakLens, a visual analysis system that enables users to explore the jailbreak performance against the target model, conduct multi-level analysis of prompt characteristics, and refine prompt instances to verify findings. Through a case study, technical evaluations, and expert interviews, we demonstrate our system's effectiveness in helping users evaluate model security and identify model weaknesses.","tags":["large language models"],"title":"JailbreakLens: Visual Analysis of Jailbreak Attacks Against Large Language Models","type":"publications"},{"authors":["Yihan Liu","Zhen Wen","Luoxuan Weng","Ollie Woodman","Yi Yang","Wei Chen"],"categories":[],"content":"","date":1718323200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718323200,"objectID":"ed1f6620fe24a26df34b4eb1d1551555","permalink":"/publications/liu-2024-sprout/","publishdate":"2024-06-14T00:00:00Z","relpermalink":"/publications/liu-2024-sprout/","section":"publications","summary":"The rapid development of large language models (LLMs), such as ChatGPT, has revolutionized the efficiency of creating programming tutorials. LLMs can be instructed with text prompts to generate comprehensive text descriptions for code snippets provided by users. However, the lack of transparency in the end-to-end generation process has hindered the understanding of model behavior and limited user control over the generated results. To tackle this challenge, we introduce a novel approach that breaks down the programming tutorial creation task into actionable steps. By employing the tree-of-thought method, LLMs engage in an exploratory process to generate diverse and faithful programming tutorials. We then present SPROUT, an authoring tool equipped with a series of interactive visualizations that empower users to have greater control and understanding of the programming tutorial creation process. A formal user study demonstrated the effectiveness of SPROUT, showing that our tool assists users to actively participate in the programming tutorial creation process, leading to more reliable and customizable results. By providing users with greater control and understanding, SPROUT enhances the user experience and improves the overall quality of programming tutorial.A free copy of this paper and all supplemental materials are available at https://osf.io/uez2t/.","tags":["large language model","programming tutorial","authoring tool","interactive visualizations"],"title":"SPROUT: an Interactive Authoring Tool for Generating Programming Tutorials with the Visualization of Large Language Models","type":"publications"},{"authors":["Jiaying Lu","Bo Pan","Jieyi Chen","Yingchaojie Feng","Jingyuan Hu","Yuchen Peng","Wei Chen"],"categories":[],"content":"","date":1714694400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714694400,"objectID":"e0df89fce64dbd2f5b88245e0292e489","permalink":"/publications/lu-2024-agentlens/","publishdate":"2024-05-03T00:00:00Z","relpermalink":"/publications/lu-2024-agentlens/","section":"publications","summary":"Recently, Large Language Model based Autonomous System (LLMAS) has gained great popularity for its potential to simulate complicated behaviors of human societies. One of its main challenges is to present and analyze the dynamic events evolution of LLMAS. In this work, we present a visualization approach to explore the detailed statuses and agents' behavior within LLMAS. Our approach outlines a general pipeline that organizes raw execution events from LLMAS into a structured behavior model. We leverage a behavior summarization algorithm to create a hierarchical summary of these behaviors, arranged according to their sequence over time. Additionally, we design a cause trace method to mine the causal relationship between agent behaviors. We then develop AgentLens , a visual analysis system that leverages a hierarchical temporal visualization for illustrating the evolution of LLMAS, and supports users to interactively investigate details and causes of agents' behaviors. Two usage scenarios and a user study demonstrate the effectiveness and usability of our AgentLens.","tags":["large language model","agent","visual analysis"],"title":"AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems","type":"publications"},{"authors":["Wei Zhang","Wong Kam-Kwai","Yitian Chen","Ailing Jia","Luwei Wang","Jian-Wei Zhang","Lechao Cheng","Huamin Qu","Wei Chen"],"categories":[],"content":"","date":1713225600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1713225600,"objectID":"4118e0712e4dd72884db9fd7492b4f16","permalink":"/publications/zhang-2024-scrolltimes/","publishdate":"2024-04-16T00:00:00Z","relpermalink":"/publications/zhang-2024-scrolltimes/","section":"publications","summary":"The study of cultural artifact provenance, tracing ownership and preservation, holds significant importance in archaeology and art history. Modern technology has advanced this field, yet challenges persist, including recognizing evidence from diverse sources, integrating sociocultural context, and enhancing interactive automation for comprehensive provenance analysis. In collaboration with art historians, we examined the handscroll, a traditional Chinese painting form that provides a rich source of historical data and a unique opportunity to explore history through cultural artifacts. We present a three-tiered methodology encompassing artifact, contextual, and provenance levels, designed to create a “Biography” for handscroll. Our approach incorporates the application of image processing techniques and language models to extract, validate, and augment elements within handscroll using various cultural heritage databases. To facilitate efficient analysis of non-contiguous extracted elements, we have developed a distinctive layout. Additionally, we introduce ScrollTimes, a visual analysis system tailored to support the three-tiered analysis of handscroll, allowing art historians to interactively create biographies tailored to their interests. Validated through case studies and expert interviews, our approach offers a window into history, fostering a holistic understanding of handscroll provenance and historical significance.","tags":["visual analytics","digital humanities","painting analysis","traditional chinese painting"],"title":"ScrollTimes: Tracing the Provenance of Paintings as a Window Into History","type":"publications"},{"authors":["Bo Pan","Jiaying Lu","Haoxuan Li","Weifeng Chen","Yiyao Wang","Minfeng Zhu","Chenhao Yu","Wei Chen"],"categories":[],"content":"","date":1697932800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697932800,"objectID":"9fa835ed81ad364ebe769bb38e97a14c","permalink":"/publications/pan-2023-differentiable/","publishdate":"2023-10-22T00:00:00Z","relpermalink":"/publications/pan-2023-differentiable/","section":"publications","summary":"The transfer function is crucial for direct volume rendering (DVR) to create an informative visual representation of volumetric data. However, manually adjusting the transfer function to achieve the desired DVR result can be time-consuming and unintuitive. In this paper, we propose Differentiable Design Galleries, an image-based transfer function design approach to help users explore the design space of transfer functions by taking advantage of the recent advances in deep learning and differentiable rendering. Specifically, we leverage neural rendering to learn a latent design space, which is a continuous manifold representing various types of implicit transfer functions. We further provide a set of interactive tools to support intuitive query, navigation, and modification to obtain the target design, which is represented as a neural-rendered design exemplar. The explicit transfer function can be reconstructed from the target design with a differentiable direct volume renderer. Experimental results on real volumetric data demonstrate the effectiveness of our method.","tags":["transfer function","direct volume rendering","deep learning","generative models","differentiable rendering"],"title":"Differentiable Design Galleries: A Differentiable Approach to Explore the Design Space of Transfer Functions","type":"publications"},{"authors":["Yingchaojie Feng","Xingbo Wang","Kam Kwai Wong","Sijia Wang","Yuhong Lu","Minfeng Zhu","Baicheng Wang","Wei Chen"],"categories":[],"content":"","date":1697932800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697932800,"objectID":"5b4f3ed0eb52b0e30d14f4c4a94a482e","permalink":"/publications/feng-2023-promptmagician/","publishdate":"2023-10-22T00:00:00Z","relpermalink":"/publications/feng-2023-promptmagician/","section":"publications","summary":"Generative text-to-image models have gained great popularity among the public for their powerful capability to generate high-quality images based on natural language prompts. However, developing effective prompts for desired images can be challenging due to the complexity and ambiguity of natural language. This research proposes PromptMagician, a visual analysis system that helps users explore the image results and refine the input prompts. The backbone of our system is a prompt recommendation model that takes user prompts as input, retrieves similar prompt-image pairs from DiffusionDB, and identifies special (important and relevant) prompt keywords. To facilitate interactive prompt refinement, PromptMagician introduces a multi-level visualization for the cross-modal embedding of the retrieved images and recommended keywords, and supports users in specifying multiple criteria for personalized exploration. Two usage scenarios, a user study, and expert interviews demonstrate the effectiveness and usability of our system, suggesting it facilitates prompt engineering and improves the creativity support of the generative text-to-image model.","tags":["prompt engineering","text-to-image generation","image visualization"],"title":"PromptMagician: Interactive Prompt Engineering for Text-to-Image Creation","type":"publications"},{"authors":["Zhen Wen","Yihan Liu","Siwei Tan","Jieyi Chen","Minfeng Zhu","Dongming Han","Jianwei Yin","Mingliang Xu","Wei Chen"],"categories":[],"content":"","date":1697932800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697932800,"objectID":"0ee99b01feb82678804268aea8d11261","permalink":"/publications/wen-2023-quantivine/","publishdate":"2023-10-22T00:00:00Z","relpermalink":"/publications/wen-2023-quantivine/","section":"publications","summary":"Quantum computing is a rapidly evolving field that enables exponential speed-up over classical algorithms. At the heart of this revolutionary technology are quantum circuits, which serve as vital tools for implementing, analyzing, and optimizing quantum algorithms. Recent advancements in quantum computing and the increasing capability of quantum devices have led to the development of more complex quantum circuits. However, traditional quantum circuit diagrams suffer from scalability and readability issues, which limit the efficiency of analysis and optimization processes. In this research, we propose a novel visualization approach for large-scale quantum circuits by adopting semantic analysis to facilitate the comprehension of quantum circuits. We first exploit meta-data and semantic information extracted from the underlying code of quantum circuits to create component segmentations and pattern abstractions, allowing for easier wrangling of massive circuit diagrams. We then develop Quantivine, an interactive system for exploring and understanding quantum circuits. A series of novel circuit visualizations are designed to uncover contextual details such as qubit provenance, parallelism, and entanglement. The effectiveness of Quantivine is demonstrated through two usage scenarios of quantum circuits with up to 100 qubits and a formal user evaluation with quantum experts. A free copy of this paper and all supplemental materials are available at https://osf.io/2m9yh/?view_only=0aa1618c97244f5093cd7ce15f1431f9.","tags":["quantum circuit","semantic analysis","visual abstraction","context visualization"],"title":"Quantivine: A Visualization Approach for Large-scale Quantum Circuit Representation and Analysis","type":"publications"},{"authors":["Wei Chen","Yating Wei","Zhiyong Wang","Shuyue Zhou","Bingru Lin","Zhiguang Zhou"],"categories":[],"content":"","date":1679961600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1679961600,"objectID":"0ddc52593ecab24197c67371ff96347d","permalink":"/publications/wei-2023-federated/","publishdate":"2023-03-28T00:00:00Z","relpermalink":"/publications/wei-2023-federated/","section":"publications","summary":"We present a novel privacy preservation strategy for aggregated visual query of decentralized data. The key idea is to imitate the flowchart of the federated learning framework, and reformulate the visualization process within a federated infrastructure. The federation of visualization is fulfilled by leveraging a shared global module that composes the encrypted externalizations of transformed visual features of data pieces in local modules. We design two implementations of federated visualization: a prediction-based scheme, and a query-based scheme. We demonstrate the effectiveness of our approach with a set of visual forms, and verify its robustness with evaluations. We report the value of federated visualization in real scenarios with an expert review.","tags":["privacy-preserving visualization","federated visualization","decentralized visualization"],"title":"Federated Visualization: A Privacy-preserving Strategy for Aggregated Visual Query ","type":"publications"},{"authors":["Jiehui Zhou","Xumeng Wang","Jie Wang","Hui Ye","Huanliang Wang","Zihan Zhou","Dongming Han","Haochao Ying","Jian Wu","Wei Chen"],"categories":[],"content":"","date":1679875200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1679875200,"objectID":"64d975c1a6d04827d51bee16daffc91c","permalink":"/publications/zhou-2023-fraudauditor/","publishdate":"2023-03-27T00:00:00Z","relpermalink":"/publications/zhou-2023-fraudauditor/","section":"publications","summary":"Collusive fraud, in which multiple fraudsters collude to defraud health insurance funds, threatens the operation of the healthcare system. However, existing statistical and machine learning-based methods have limited ability to detect fraud in the scenario of health insurance due to the high similarity of fraudulent behaviors to normal medical visits and the lack of labeled data. To ensure the accuracy of the detection results, expert knowledge needs to be integrated with the fraud detection process. By working closely with health insurance audit experts, we propose FraudAuditor, a three-stage visual analytics approach to collusive fraud detection in health insurance. Specifically, we first allow users to interactively construct a co-visit network to holistically model the visit relationships of different patients. Second, an improved community detection algorithm that considers the strength of fraud likelihood is designed to detect suspicious fraudulent groups. Finally, through our visual interface, users can compare, investigate, and verify suspicious patient behavior with tailored visualizations that support different time scales. We conducted case studies in a real-world healthcare scenario, i.e., to help locate the actual fraud group and exclude the false positive group. The results and expert feedback proved the effectiveness and usability of the approach.","tags":["collusive fraud","fraud detection","health insurance","visual analytics"],"title":"FraudAuditor: A Visual Analytics Approach for Collusive Fraud in Health Insurance","type":"publications"},{"authors":["Yating Wei","Zhiyong Wang","Zhongwei Wang","Yong Dai","Gongchang Ou","Han Gao","Haitao Yang","Yue Wang","Caleb Chen Cao","Luoxuan Weng","Jiaying Lu","Rongchen Zhu","Wei Chen"],"categories":[],"content":"","date":1675900800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675900800,"objectID":"184959098556c1cba640c21a1f6e826a","permalink":"/publications/wei-2023-visual/","publishdate":"2023-02-09T00:00:00Z","relpermalink":"/publications/wei-2023-visual/","section":"publications","summary":"Diagnosing the cluster-based performance of large-scale deep neural network (DNN) models during training is essential for improving training efficiency and reducing resource consumption. However, it remains challenging due to the incomprehensibility of the parallelization strategy and the sheer volume of complex data generated in the training processes. Prior works visually analyze performance profiles and timeline traces to identify anomalies from the perspective of individual devices in the cluster, which is not amenable for studying the root cause of anomalies. In this paper, we present a visual analytics approach that empowers analysts to visually explore the parallel training process of a DNN model and interactively diagnose the root cause of a performance issue. A set of design requirements is gathered through discussions with domain experts. We propose an enhanced execution flow of model operators for illustrating parallelization strategies within the computational graph layout. We design and implement an enhanced Marey's graph representation, which introduces the concept of time-span and a banded visual metaphor to convey training dynamics and help experts identify inefficient training processes. We also propose a visual aggregation technique to improve visualization efficiency. We evaluate our approach using case studies, a user study and expert interviews on two large-scale models run in a cluster, namely, the PanGu-α 13B model (40 layers), and the Resnet model (50 layers).","tags":["deep neural network","model training","parallel performance"],"title":"Visual Diagnostics of Parallel Performance in Training Large-Scale DNN Models","type":"publications"},{"authors":["Yingchaojie Feng","Xingbo Wang","Bo Pan","Kam Kwai Wong","Yi Ren","Shi Liu","Zihan Yan","Yuxin Ma","Huamin Qu","Wei Chen"],"categories":[],"content":"","date":1674691200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1674691200,"objectID":"a4ead00cd3d4a34da6c76cc5f4849b32","permalink":"/publications/xnli-explaining-and-diagnosing-nli-based-visual-data-analysis/","publishdate":"2023-01-26T00:00:00Z","relpermalink":"/publications/xnli-explaining-and-diagnosing-nli-based-visual-data-analysis/","section":"publications","summary":"Natural language interfaces (NLIs) enable users to flexibly specify analytical intentions in data visualization. However, diagnosing the visualization results without understanding the underlying generation process is challenging. Our research explores how to provide explanations for NLIs to help users locate the problems and further revise the queries. We present XNLI, an explainable NLI system for visual data analysis. The system introduces a Provenance Generator to reveal the detailed process of visual transformations, a suite of interactive widgets to support error adjustments, and a Hint Generator to provide query revision hints based on the analysis of user queries and interactions. Two usage scenarios of XNLI and a user study verify the effectiveness and usability of the system. Results suggest that XNLI can significantly enhance task accuracy without interrupting the NLI-based analysis process.","tags":["natural language interface","visual data analysis","explainability"],"title":"XNLI: Explaining and Diagnosing NLI-based Visual Data Analysis","type":"publications"},{"authors":["Wei Zhang","Jason K. Wong","Xumeng Wang","Youcheng Gong","Rongchen Zhu","Kai Liu","Zihan Yan","Siwei Tan","Huamin Qu","Siming Chen","Wei Chen"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"fd8df6fe16e6c00ba5ebabc1cc6a7ac8","permalink":"/publications/cohortva-a-visual-analytic-system-for-interactive-exploration-of-cohorts-based-on-historical-data/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publications/cohortva-a-visual-analytic-system-for-interactive-exploration-of-cohorts-based-on-historical-data/","section":"publications","summary":"In history research, cohort analysis seeks to identify social structures and figure mobilities by studying the group-based behavior of historical figures. Prior works mainly employ automatic data mining approaches, lacking effective visual explanation. In this paper, we present CohortVA, an interactive visual analytic approach that enables historians to incorporate expertise and insight into the iterative exploration process. The kernel of CohortVA is a novel identification model that generates candidate cohorts and constructs cohort features by means of pre-built knowledge graphs constructed from large-scale history databases. We propose a set of coordinated views to illustrate identified cohorts and features coupled with historical events and figure profiles. Two case studies and interviews with historians demonstrate that CohortVA can greatly enhance the capabilities of cohort identifications, figure authentications, and hypothesis generation.","tags":["historical cohort analysis","machine learning","interpretability"],"title":"CohortVA: A Visual Analytic System for Interactive Exploration of Cohorts based on Historical Data","type":"publications"},{"authors":["Jiehui Zhou","Xumeng Wang","Jason K. Wong","Huanliang Wang","Zhongwei Wang","Xiaoyu Yang","Xiaoran Yan","Haozhe Feng","Huamin Qu","Haochao Ying","Wei Chen"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"94e2d1c6049d635089eaea2300b24325","permalink":"/publications/zhou-2023-dpviscreator/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publications/zhou-2023-dpviscreator/","section":"publications","summary":"Data privacy is an essential issue in publishing data visualizations. However, it is challenging to represent multiple data patterns in privacy-preserving visualizations. The prior approaches target specific chart types or perform an anonymization model uniformly without considering the importance of data patterns in visualizations. In this paper, we propose a visual analytics approach that facilitates data custodians to generate multiple private charts while maintaining user-preferred patterns. To this end, we introduce pattern constraints to model users' preferences over data patterns in the dataset and incorporate them into the proposed Bayesian network-based Differential Privacy (DP) model PriVis . A prototype system, DPVisCreator , is developed to assist data custodians in implementing our approach. The effectiveness of our approach is demonstrated with quantitative evaluation of pattern utility under the different levels of privacy protection, case studies, and semi-structured expert interviews.","tags":["privacy-preserving visualization","differential privacy","tabular data","visual analytics"],"title":"DPVisCreator: Incorporating Pattern Constraints to Privacy-preserving Visualizations via Differential Privacy","type":"publications"},{"authors":["Zhen Wen","Wei Zeng","Luoxuan Weng","Yihan Liu","Mingliang Xu","Wei Chen"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"84a47ddc8d3aba923ddefc844adba7df","permalink":"/publications/situated-multiple-view/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publications/situated-multiple-view/","section":"publications","summary":"Multiple-view (MV) representations enabling multi-perspective exploration of large and complex data are often employed on 2D displays. The technique also shows great potential in addressing complex analytic tasks in immersive visualization. However, although useful, the design space of MV representations in immersive visualization lacks in deep exploration. In this paper, we propose a new perspective to this line of research, by examining the effects of view layout for MV representations on situated analytics. Specifically, we disentangle situated analytics in perspectives of situatedness regarding spatial relationship between visual representations and physical referents, and analytics regarding cross-view data analysis including filtering, refocusing, and connecting tasks. Through an in-depth analysis of existing layout paradigms, we summarize design trade-offs for achieving high situatedness and effective analytics simultaneously. We then distill a list of design requirements for a desired layout that balances situatedness and analytics, and develop a prototype system with an automatic layout adaptation method to fulfill the requirements. The method mainly includes a cylindrical paradigm for egocentric reference frame, and a force-directed method for proper view-view, view-user, and view-referent proximities and high view visibility. We conducted a formal user study that compares layouts by our method with linked and embedded layouts. Quantitative results show that participants finished filtering- and connecting-centered tasks significantly faster with our layouts, and user feedback confirms high usability of the prototype system.","tags":["situated analytics","multiple-view representations","view layout","immersive visualization"],"title":"Effects of View Layout on Situated Analytics for Multiple-View Representations in Immersive Visualization","type":"publications"},{"authors":["Xumeng Wang","Wei Chen","Jiazhi Xia","Zhen Wen","Rongchen Zhu","Tobias Schreck"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"c941104967c75bf74b993307e2358db3","permalink":"/publications/hetvis-a-visual-analysis-approach-for-identifying-data-heterogeneity-in-horizontal-federated-learning/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/publications/hetvis-a-visual-analysis-approach-for-identifying-data-heterogeneity-in-horizontal-federated-learning/","section":"publications","summary":"Horizontal federated learning (HFL) enables distributed clients to train a shared model and keep their data privacy. In training high-quality HFL models, the data heterogeneity among clients is one of the major concerns. However, due to the security issue and the complexity of deep learning models, it is challenging to investigate data heterogeneity across different clients. To address this issue, based on a requirement analysis we developed a visual analytics tool, HetVis, for participating clients to explore data heterogeneity. We identify data heterogeneity through comparing prediction behaviors of the global federated model and the stand-alone model trained with local data. Then, a context-aware clustering of the inconsistent records is done, to provide a summary of data heterogeneity. Combining with the proposed comparison techniques, we develop a novel set of visualizations to identify heterogeneity issues in HFL. We designed three case studies to introduce how HetVis can assist client analysts in understanding different types of heterogeneity issues. Expert reviews and a comparative study demonstrate the effectiveness of HetVis.","tags":["federated learning","data heterogeneity","cluster analysis","visual analysis"],"title":"HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning","type":"publications"},{"authors":["Rusheng Pan","Zhiyong Wang","Yating Wei","Han Gao","Gongchang Ou","Caleb Chen Cao","Jingli Xu","Tong Xu","Wei Chen"],"categories":[],"content":"","date":1672012800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672012800,"objectID":"a9e77a0475f909aa3d02c8ca1d40bc5f","permalink":"/publications/pan-2022-towards/","publishdate":"2022-12-26T00:00:00Z","relpermalink":"/publications/pan-2022-towards/","section":"publications","summary":"A computational graph in a deep neural network (DNN) denotes a specific data flow diagram (DFD) composed of many tensors and operators. Existing toolkits for visualizing computational graphs are not applicable when the structure is highly complicated and large-scale (e.g., BERT). To address this problem, we propose leveraging a suite of visual simplification techniques, including a cycle-removing method, a module-based edge-pruning algorithm, and an isomorphic subgraph stacking strategy. We design and implement an interactive visualization system that is suitable for computational graphs with up to 10 thousand elements. Experimental results and usage scenarios demonstrate that our tool reduces 60% elements on average and hence enhances the performance for recognizing and diagnosing DNN models. Our contributions are integrated into an open-source DNN visualization toolkit, namely, MindInsight.","tags":["deep neural networks","computational graphs","graph visualization","graph layout","visual simplifications"],"title":"Towards Efficient Visual Simplification of Computational Graphs in Deep Neural Networks","type":"publications"},{"authors":["Jianwei Zhang","Yifan Sun","Yi Yang","Wei Chen"],"categories":[],"content":"","date":1667260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667260800,"objectID":"c25e64cad5eb2d88b8ff1e48e2675611","permalink":"/publications/fptrans/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/publications/fptrans/","section":"publications","summary":"Few-shot segmentation~(FSS) aims at performing semantic segmentation on novel classes given a few annotated support samples. With a rethink of recent advances, we find that the current FSS framework has deviated far from the supervised segmentation framework: Given the deep features, FSS methods typically use an intricate decoder to perform sophisticated pixel-wise matching, while the supervised segmentation methods use a simple linear classification head. Due to the intricacy of the decoder and its matching pipeline, it is not easy to follow such an FSS framework. This paper revives the straightforward framework of \"feature extractor linear classification head\" and proposes a novel Feature-Proxy Transformer (FPTrans) method, in which the \"proxy\" is the vector representing a semantic class in the linear classification head. FPTrans has two keypoints for learning discriminative features and representative proxies: 1) To better utilize the limited support samples, the feature extractor makes the query interact with the support features from bottom to top layers using a novel prompting strategy. 2) FPTrans uses multiple local background proxies (instead of a single one) because the background is not homogeneous and may contain some novel foreground regions. These two keypoints are easily integrated into the vision transformer backbone with the prompting mechanism in the transformer. Given the learned features and proxies, FPTrans directly compares their cosine similarity for segmentation. Although the framework is straightforward, we show that FPTrans achieves competitive FSS accuracy on par with state-of-the-art decoder-based methods.","tags":["few-shot segmentation","vision transformer","prompt learning"],"title":"Feature-Proxy Transformer for Few-Shot Segmentation","type":"publications"},{"authors":["Yijing Liu","Qinxian Liu","Jianwei Zhang","Haozhe Feng","Zhongwei Wang","Zihan Zhou","Wei Chen"],"categories":[],"content":"","date":1667260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667260800,"objectID":"1cd325ec87006e2fe68381fa7f4316bd","permalink":"/publications/tpgnn/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/publications/tpgnn/","section":"publications","summary":"Modeling multivariate time series (MTS) is critical in modern intelligent systems. The accurate forecast of MTS data is still challenging due to the complicated latent variable correlation. Recent works apply the Graph Neural Networks (GNNs) to the task, with the basic idea of representing the correlation as a static graph. However, predicting with a static graph causes significant bias because the correlation is time-varying in the real-world MTS data. Besides, there is no gap analysis between the actual correlation and the learned one in their works to validate the effectiveness. This paper proposes a temporal polynomial graph neural network (TPGNN) for accurate MTS forecasting, which represents the dynamic variable correlation as a temporal matrix polynomial in two steps. First, we capture the overall correlation with a static matrix basis. Then, we use a set of time-varying coefficients and the matrix basis to construct a matrix polynomial for each time step. The constructed result empirically captures the precise dynamic correlation of six synthetic MTS datasets generated by a non-repeating random walk model. Moreover, the theoretical analysis shows that TPGNN can achieve perfect approximation under a commutative condition. We conduct extensive experiments on two traffic datasets with prior structure and four benchmark datasets. The results indicate that TPGNN achieves the state-of-the-art on both short-term and long-term MTS forecastings.","tags":["graph neural networks","multivariate time-series forecasting","spatial-temporal graph"],"title":"Multivariate Time-Series Forecasting with Temporal Polynomial Graph Neural Networks","type":"publications"},{"authors":["Xumeng Wang","Chris Bryan","Yiran Li","Rusheng Pan","Yanling Liu","Wei Chen","Kwan-Liu Ma"],"categories":[],"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656633600,"objectID":"b51463db7327bb2f1dd4fdbdf9a467a3","permalink":"/publications/umbra-a-visual-analysis-approach-for-defense-construction-against-inference-attacks-on-sensitive-information/","publishdate":"2022-07-01T00:00:00Z","relpermalink":"/publications/umbra-a-visual-analysis-approach-for-defense-construction-against-inference-attacks-on-sensitive-information/","section":"publications","summary":"Collecting and analyzing anonymous personal information is required as a part of data analysis processes, such as medical diagnosis and restaurant recommendation. Such data should ostensibly be stored so that specific individual information cannot be disclosed. Unfortunately, inference attacks—integrating background knowledge and intelligent models—hinder classic sanitization techniques like syntactic anonymity and differential privacy from exhaustively protecting sensitive information. As a solution, we introduce a three-stage approach empowered within a visual interface, which depicts underlying inference behaviors via a Bayesian Network and supports a customized defense against inference attacks from unknown adversaries. In particular, our approach visually explains the process details of the underlying privacy preserving models, allowing users to verify if the results sufficiently satisfy the requirements of privacy preservation. We demonstrate the effectiveness of our approach through two case studies and expert reviews.","tags":["privacy","inference attack","bayesian network"],"title":"Umbra: A Visual Analysis Approach for Defense Construction Against Inference Attacks on Sensitive Information","type":"publications"},{"authors":["Wei Zhang","Siwei Tan","Siming Chen","Linhao Meng","Tianye Zhang","Rongchen Zhu","Wei Chen"],"categories":[],"content":"","date":1642377600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642377600,"objectID":"d0577cf6679d4cf367fd7b8a68233280","permalink":"/publications/visual-reasoning-for-uncertainty-in-spatio-temporal-events-of-historical-figures/","publishdate":"2022-01-17T00:00:00Z","relpermalink":"/publications/visual-reasoning-for-uncertainty-in-spatio-temporal-events-of-historical-figures/","section":"publications","summary":"The development of digitized humanity information provides a new perspective on data-oriented studies of history. Many previous studies have ignored uncertainty in the exploration of historical figures and events, which has limited the capability of researchers to capture complex processes associated with historical phenomena. We propose a visual reasoning system to support visual reasoning of uncertainty associated with spatio-temporal events of historical figures based on data from the China Biographical Database Project. We build a knowledge graph of entities extracted from a historical database to capture uncertainty generated by missing data and error. The proposed system uses an overview of chronology, a map view, and an interpersonal relation matrix to describe and analyse heterogeneous information of events. The system also includes uncertainty visualization to identify uncertain events with missing or imprecise spatio-temporal information. Results from case studies and expert evaluations suggest that the visual reasoning system is able to quantify and reduce uncertainty generated by the data.","tags":["history","uncertainty","spatio-temporal events","visual reasoning"],"title":"Visual Reasoning for Uncertainty in Spatio-temporal Events of Historical Figures","type":"publications"},{"authors":["Tianye Zhang","Haozhe Feng","Wei Chen","Zexian Chen","Wenting Zheng","Xiaonan Luo","Wenqi Huang","Anthony Tung"],"categories":[],"content":"","date":1625184000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625184000,"objectID":"492b8dc6135cb22e0db4caa87a07b374","permalink":"/publications/chartnavigator/","publishdate":"2021-07-02T00:00:00Z","relpermalink":"/publications/chartnavigator/","section":"publications","summary":"Patterns in charts refer to interesting visual features or forms. Identifying patterns not only helps analysts understand the 'shape' of the data but also supports better and faster decision-making. Existing solutions for identifying patterns in charts require a large number of labeled data instances, making it intractable without user supervision. In this paper, we propose ChartNavigator, an interactive pattern identification and annotation framework for unlabeled visualization charts. ChartNavigator leverages a novel chart-sensitive deep factor model to map patterns into a low-dimensional factor representation space, and facilitates rich analysis with the derived representations. We design and implement a visual interface to support efficient identification and annotation of potential patterns in charts. Evaluations with multiple datasets show that our approach outperforms the baseline models in identifying and annotating patterns.","tags":["pattern identification","variational autoencoder","chart recognition"],"title":"ChartNavigator: An Interactive Pattern Identification and Annotation Framework for Charts","type":"publications"},{"authors":["Haozhe Feng","Kezhi Kong","Minghao Chen","Tianye Zhang","Minfeng Zhu","Wei Chen"],"categories":[],"content":"","date":1621296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621296000,"objectID":"6d4eefd84726dc758bf2f1cc2a7d0411","permalink":"/publications/shot-vae/","publishdate":"2021-05-18T00:00:00Z","relpermalink":"/publications/shot-vae/","section":"publications","summary":"Semi-supervised variational autoencoders (VAEs) have obtained strong results, but have also encountered the challenge that good ELBO values do not always imply accurate inference results.In this paper, we investigate and propose two causes of this problem: (1) The ELBO objective cannot utilize the label information directly. (2) A bottleneck value exists, and continuing to optimize ELBO after this value will not improve inference accuracy. On the basis of the experiment results, we propose SHOT-VAE to address these problems without introducing additional prior knowledge. The SHOT-VAE offers two contributions: (1) A new ELBO approximation named smooth-ELBO that integrates the label predictive loss into ELBO. (2) An approximation based on optimal interpolation that breaks the ELBO value bottleneck by reducing the margin between ELBO and the data likelihood. The SHOT-VAE achieves good performance with 25.30% error rate on CIFAR-100 with 10k labels and reduces the error rate to 6.11% on CIFAR-10 with 4k labels.","tags":["Node-link diagram","graph layout","graph visualization","user interactions"],"title":"SHOT-VAE: Semi-supervised Deep Generative Models With Label-aware ELBO Approximations","type":"publications"},{"authors":["Haozhe Feng","Zhaoyang You","Minghao Chen","Tianye Zhang","Minfeng Zhu","Fei Wu","Chao Wu","Wei Chen"],"categories":[],"content":"","date":1620432000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620432000,"objectID":"ff2f4feac24fd33344136b781f6a7317","permalink":"/publications/kd3a/","publishdate":"2021-05-08T00:00:00Z","relpermalink":"/publications/kd3a/","section":"publications","summary":"Conventional unsupervised multi-source domain adaptation (UMDA) methods assume all source domains can be accessed directly. This neglects the privacy-preserving policy, that is, all the data and computations must be kept decentralized. There exists three problems in this scenario: (1) Minimizing the domain distance requires the pairwise calculation of the data from source and target domains, which is not accessible. (2) The communication cost and privacy security limit the application of UMDA methods (e.g., the domain adversarial training). (3) Since users have no authority to check the data quality, the irrelevant or malicious source domains are more likely to appear, which causes negative transfer. In this study, we propose a privacy-preserving UMDA paradigm named Knowledge Distillation based Decentralized Domain Adaptation (KD3A), which performs domain adaptation through the knowledge distillation on models from different source domains. KD3A solves the above problems with three components: (1) A multi-source knowledge distillation method named Knowledge Vote to learn high-quality domain consensus knowledge. (2) A dynamic weighting strategy named Consensus Focus to identify both the malicious and irrelevant domains. (3) A decentralized optimization strategy for domain distance named BatchNorm MMD. The extensive experiments on DomainNet demonstrate that KD3A is robust to the negative transfer and brings a 100x reduction of communication cost compared with other decentralized UMDA methods. Moreover, our KD3A significantly outperforms state-of-the-art UMDA approaches.","tags":["Federated Learning","Domain Adaptation"],"title":"KD3A: Unsupervised Multi-Source Decentralized Domain Adaptation via Knowledge Distillation","type":"publications"},{"authors":["Minfeng Zhu"],"categories":null,"content":"","date":1603826120,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603826120,"objectID":"ee20f634ea3ee63157c511a8fa08f1f6","permalink":"/talks/2020vis-minfeng/","publishdate":"2020-10-26T08:00:00+08:00","relpermalink":"/talks/2020vis-minfeng/","section":"talks","summary":"TVCG Paper: DRGraph: An Efficient Graph Layout Algorithm for Large-scale Graphs by Dimensionality Reduction.","tags":["VIS2020"],"title":"Minfeng Zhu presenting TVCG Paper “DRGraph: An Efficient Graph Layout Algorithm for Large-scale Graphs by Dimensionality Reduction” at IEEE InfoVis 2020","type":"talks"},{"authors":[],"categories":null,"content":"","date":1603825220,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603825220,"objectID":"12c9c95b7ca1bb6c06ff03b9ec814158","permalink":"/talks/2020vis-xumeng/","publishdate":"2020-10-26T08:00:00+08:00","relpermalink":"/talks/2020vis-xumeng/","section":"talks","summary":"VAST Paper: ConceptExplorer: Visual Analysis of Concept Drifts in Multi-source Time-series Data.","tags":["VIS2020"],"title":"Xumeng Wang presenting VAST Paper “ConceptExplorer: Visual Analysis of Concept Drifts in Multi-source Time-series Data” at IEEE VAST 2020","type":"talks"},{"authors":["Jiacheng Pan"],"categories":null,"content":"","date":1603670400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603670400,"objectID":"ed3d1a8b1dcea4bd3254c37327df26f6","permalink":"/talks/2020vis-jiacheng/","publishdate":"2020-10-26T08:00:00+08:00","relpermalink":"/talks/2020vis-jiacheng/","section":"talks","summary":"TVCG Paper: Exemplar-based Layout Fine-tuning for Node-link Diagrams.","tags":["VIS2020"],"title":"Jiacheng Pan presenting TVCG Paper “Exemplar-based Layout Fine-tuning for Node-link Diagrams” at IEEE InfoVis 2020","type":"talks"},{"authors":["Xumeng Wang","Wei Chen","Jiazhi Xia","Zexian Chen","Dongshi Xu","Xiangyang Wu","Mingliang Xu","Tobias Schreck"],"categories":[],"content":"","date":1597449600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597449600,"objectID":"16481da1f6d2f1d6d084ce0fb45e64fd","permalink":"/publications/conceptexplorer/","publishdate":"2020-08-15T00:00:00Z","relpermalink":"/publications/conceptexplorer/","section":"publications","summary":"Time-series data is widely studied in various scenarios, like weather forecast, stock market, customer behavior analysis. To comprehensively learn about the dynamic environments, it is necessary to comprehend features from multiple data sources. This paper proposes a novel visual analysis approach for detecting and analyzing concept drifts from multi-sourced time-series.  We propose a visual detection scheme for discovering concept drifts from multiple sourced time-series based on prediction models. We design a drift level index to depict the dynamics, and a consistency judgment model to justify whether the concept drifts from various sources are consistent. Our integrated visual interface, ConceptExplorer, facilitates visual exploration, extraction, understanding, and comparison of concepts and concept drifts from multi-source time-series data.  We conduct three case studies and expert interviews to verify the effectiveness of our approach.","tags":["Temporal Data","Data Analysis, Reasoning, Problem Solving, and Decision Making","Machine Learning Techniques"],"title":"ConceptExplorer: Visual Analysis of Concept Drifts in Multi-source Time-series Data","type":"publications"},{"authors":["Minfeng Zhu","Wei Chen","Yuanzhe Hu","Yuxuan Hou","Liangjun Liu","Kaiyuan Zhang"],"categories":[],"content":"","date":1597449600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597449600,"objectID":"cc5bb3718596c9bdcbc69405e2d8763d","permalink":"/publications/drgraph/","publishdate":"2020-08-15T00:00:00Z","relpermalink":"/publications/drgraph/","section":"publications","summary":"Efficient layout of large-scale graphs remains a challenging problem: the force-directed and dimensionality reduction-based methods suffer from high overhead for graph distance and gradient computation. In this paper, we present a new graph layout algorithm, called DRGraph, that enhances the nonlinear dimensionality reduction process with three schemes: approximating graph distances by means of a sparse distance matrix, estimating the gradient by using the negative sampling technique, and accelerating the optimization process through a multi-level layout scheme. DRGraph achieves a linear complexity for the computation and memory consumption, and scales up to large-scale graphs with millions of nodes. Experimental results and comparisons with state-of-the-art graph layout methods demonstrate that DRGraph can generate visually comparable layouts with a faster running time and a lower memory requirement.","tags":[],"title":"DRGraph: An Efficient Graph Layout Algorithm for Large-scale Graphs by Dimensionality Reduction","type":"publications"},{"authors":["Jiacheng Pan","Wei Chen","Xiaodong Zhao","Shuyue Zhou","Wei Zeng","Minfeng Zhu","Jian Chen","Siwei Fu","Yingcai Wu"],"categories":[],"content":"","date":1597449600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597449600,"objectID":"5b16d7aa8dd82bd82259863f8581e9c0","permalink":"/publications/exemplar-based-fine-tuning/","publishdate":"2020-08-15T00:00:00Z","relpermalink":"/publications/exemplar-based-fine-tuning/","section":"publications","summary":"We design and evaluate a novel layout fine-tuning technique for node-link diagrams that facilitates exemplar-based adjustment of a group of substructures in batching mode. The key idea is to transfer user modifications on a local substructure to other substructures in the whole graph that are topologically similar to the exemplar. We first precompute a canonical representation for each substructure with node embedding techniques and then use it for on-the-fly substructure retrieval. We design and develop a light-weight interactive system to enable intuitive adjustment, modification transfer, and visual graph exploration. We also report some results of quantitative comparisons, three case studies, and a within-participant user study.","tags":["Node-link diagram","graph layout","graph visualization","user interactions"],"title":"Exemplar-based Layout Fine-tuning for Node-link Diagrams","type":"publications"},{"authors":["Ruixian Ma","Honghui Mei","Huihua Guan","Wei Huang","Fan Zhang","Changye Xin","Wenzhuo Dai","Xiao Wen","Wei Chen"],"categories":[],"content":"","date":1584057600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584057600,"objectID":"b26c55292b08e762b74d882b25594a28","permalink":"/publications/ladv/","publishdate":"2020-03-13T00:00:00Z","relpermalink":"/publications/ladv/","section":"publications","summary":"Dashboard visualizations are widely used in data-intensive applications such as business intelligence, operation monitoring, and urban planning. However, existing visualization authoring tools are inefficient in the rapid prototyping of dashboards because visualization expertise and user intention need to be integrated. We propose a novel approach to rapid conceptualization that can construct dashboard templates from exemplars to mitigate the burden of designing, implementing, and evaluating dashboard visualizations. The kernel of our approach is a novel deep learning-based model that can identify and locate charts of various categories and extract colors from an input image or sketch. We design and implement a web-based authoring tool for learning, composing, and customizing dashboard visualizations in a cloud computing environment. Examples, user studies, and user feedback from real scenarios in Alibaba Cloud verify the usability and efficiency of the proposed approach.","tags":["Dashboard visualization","visual design","stylization","deep learning-based"],"title":"LADV: Deep Learning Assisted Authoring of Dashboard Visualizations from Images and Sketches","type":"publications"},{"authors":["Minfeng Zhu","Pingbo Pan","Wei Chen","Yi Yang"],"categories":[],"content":"","date":1574726400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574726400,"objectID":"deeb17c52aa5b36202f2915f7256a02e","permalink":"/publications/eemefn/","publishdate":"2019-11-26T00:00:00Z","relpermalink":"/publications/eemefn/","section":"publications","summary":"This work focuses on the extremely low-light image enhance-ment, which aims to improve image brightness and revealhidden information in darken areas. Recently, image enhance-ment approaches have yielded impressive progress. However,existing methods still suffer from three main problems: (1)low-light images usually are high-contrast. Existing methodsmay fail to recover images details in extremely dark or brightareas; (2) current methods cannot precisely correct the colorof low-light images; (3) when the object edges are unclear,the pixel-wise loss may treat pixels of different objectsequally and produce blurry images. In this paper, we proposea two-stage method called Edge-Enhanced Multi-ExposureFusion Network (EEMEFN) to enhance extremely low-lightimages. In the first stage, we employ a multi-exposure fusionmodule to address the high contrast and color bias issues. Wesynthesize a set of images with different exposure time froma single image and construct an accurate normal-light imageby combining well-exposed areas under different illuminationconditions. Thus, it can produce realistic initial images withcorrect color from extremely noisy and low-light images.Secondly, we introduce an edge enhancement module torefine the initial images with the help of the edge information.Therefore, our method can reconstruct high-quality imageswith sharp edges when minimizing the pixel-wise loss. Ex-periments on the See-in-the-Dark dataset indicate that ourEEMEFN approach achieves state-of-the-art performance.","tags":[],"title":"EEMEFN: Low-Light Image Enhancement via Edge-Enhanced Multi-Exposure Fusion Network","type":"publications"},{"authors":["Zhaosong Huang"],"categories":null,"content":"","date":1571967020,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571967020,"objectID":"55b457acb770bc5a56b674ec9b3955d9","permalink":"/talks/2019vis-zhaosong-1/","publishdate":"2019-10-01T20:05:20+08:00","relpermalink":"/talks/2019vis-zhaosong-1/","section":"talks","summary":"VAST TVCG Paper: A Natural-language-based Visual Query Approach of Uncertain Human Trajectories.","tags":["VIS2019"],"title":"Zhaosong Huang presenting VAST TVCG Paper “A Natural-language-based Visual Query Approach of Uncertain Human Trajectories” at IEEE VAST 2019","type":"talks"},{"authors":["Zhaosong Huang"],"categories":null,"content":"","date":1571902520,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571902520,"objectID":"f12100efd5b1bea2ab466152d0e78b84","permalink":"/talks/2019vis-zhaosong-2/","publishdate":"2019-10-01T20:05:20+08:00","relpermalink":"/talks/2019vis-zhaosong-2/","section":"talks","summary":"TVCG Paper: Exploring the Sensitivity of Choropleths under Attribute Uncertainty.","tags":["VIS2019"],"title":"Zhaosong Huang presenting TVCG Paper “Exploring the Sensitivity of Choropleths under Attribute Uncertainty” at IEEE VAST 2019","type":"talks"},{"authors":["Yating Wei"],"categories":null,"content":"","date":1571735420,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571735420,"objectID":"219d73378e3e708c9952d17f8f26293a","permalink":"/talks/2019vis-yating/","publishdate":"2019-08-12T20:05:20+08:00","relpermalink":"/talks/2019vis-yating/","section":"talks","summary":"VAST 2019: Evaluating Perceptual Bias During Geometric Scaling of Scatterplots","tags":["VIS2019"],"title":"Yating Wei presenting Evaluating Perceptual Bias During Geometric Scaling of Scatterplots at IEEE VAST 2019","type":"talks"},{"authors":["Honghui Mei","Wei Chen","Yating Wei","Yuanzhe Hu","Shuyue Zhou","Bingru Lin","Ying Zhao","Jiazhi Xia"],"categories":[],"content":"","date":1565049600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565049600,"objectID":"9b821474a23b5adb131ea562020c445c","permalink":"/publications/rsa-tree/","publishdate":"2019-08-06T00:00:00Z","relpermalink":"/publications/rsa-tree/","section":"publications","summary":"Analysts commonly investigate the data distributions derived from statistical aggregations of data that are represented by charts, such as histograms and binned scatterplots, to visualize and analyze a large-scale dataset. Aggregate queries are implicitly executed through such a process. Datasets are constantly extremely large; thus, the response time should be accelerated by calculating predefined data cubes. However, the queries are limited to the predefined binning schema of preprocessed data cubes. Such limitation hinders analysts' flexible adjustment of visual specifications to investigate the implicit patterns in the data effectively. Particularly, RSATree enables arbitrary queries and flexible binning strategies by leveraging three schemes, namely, an R-tree-based space partitioning scheme to catch the data distribution, a locality-sensitive hashing technique to achieve locality-preserving random access to data items, and a summed area table scheme to support interactive query of aggregated values with a linear computational complexity. This study presents and implements a web-based visual query system that supports visual specification, query, and exploration of large-scale tabular data with user-adjustable granularities. We demonstrate the efficiency and utility of our approach by performing various experiments on real-world datasets and analyzing time and space complexity.","tags":["Aggregate query","visual query","large-scale data visualization","R-tree","summed area table","hashing"],"title":"RSATree: Distribution-AwareData Representation of Large-Scale Tabular Datasets for Flexible Visual Query","type":"publications"},{"authors":["Zhaosong Huang","Ye Zhao","Wei Chen","Shengjie Gao","Kejie Yu","Weixia Xu","Mingjie Tang","Minfeng Zhu","Mingliang Xu"],"categories":[],"content":"","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"664b6a4cbe2e95a5319c3cabfaef4e7c","permalink":"/publications/nlp-urban/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/publications/nlp-urban/","section":"publications","summary":"Visual querying is essential for interactively exploring massive trajectory data. However, the data uncertainty imposes profound challenges to fulfill advanced analytics requirements. On the one hand, many underlying data does not contain accurate geographic coordinates, e.g., positions of a mobile phone only refer to the regions (i.e., mobile cell stations) in which it resides, instead of accurate GPS coordinates. On the other hand, domain experts and general users prefer a natural way, such as using a natural language sentence, to access and analyze massive movement data. In this paper, we propose a visual analytics approach that can extract spatial-temporal constraints from a textual sentence and support an effective query method over uncertain mobile trajectory data. It is built up on encoding massive, spatially uncertain trajectories by the semantic information of the POIs and regions covered by them, and then storing the trajectory documents in text database with an effective indexing scheme. The visual interface facilitates query condition specification, situation-aware visualization, and semantic exploration of large trajectory data. Usage scenarios on real-world human mobility datasets demonstrate the effectiveness of our approach.","tags":["Natural-language-based Visual Query","Spatial Uncertaity","Trajectory Exploration"],"title":"A Natural-language-based Visual Query Approach of Uncertain Human Trajectories","type":"publications"},{"authors":["Yating Wei","Honghui Mei","Ying Zhao","Shuyue Zhou","Bingru Lin","Haojing Jiang","Wei Chen"],"categories":[],"content":"","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"a47184a8962ab5d03ded333d295fbd8f","permalink":"/publications/evaluating-scatterplots/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/publications/evaluating-scatterplots/","section":"publications","summary":"Scatterplots are frequently scaled to fit display areas in multi-view and multi-device data analysis environments. A common method used for scaling is to enlarge or shrink the entire scatterplot together with the inside points synchronously and proportionally. This process is called geometric scaling. However, geometric scaling of scatterplots may cause a perceptual bias, that is, the perceived and physical values of visual features may be dissociated with respect to geometric scaling. For example, if a scatterplot is projected from a laptop to a large projector screen, then observers may feel that the scatterplot shown on the projector has fewer points than that viewed on the laptop. This paper presents an evaluation study on the perceptual bias of visual features in scatterplots caused by geometric scaling. The study focuses on three fundamental visual features (i.e., numerosity, correlation, and cluster separation) and three hypotheses that are formulated on the basis of our experience. We carefully design three controlled experiments by using well-prepared synthetic data and recruit participants to complete the experiments on the basis of their subjective experience. With a detailed analysis of the experimental results, we obtain a set of instructive findings. First, geometric scaling causes a bias that has a linear relationship with the scale ratio. Second, no significant difference exists between the biases measured from normally and uniformly distributed scatterplots. Third, changing the point radius can correct the bias to a certain extent. These findings can be used to inspire the design decisions of scatterplots in various scenarios.","tags":["Evaluation","scatterplot","geometric scaling","bias","perceptual consistency"],"title":"Evaluating Perceptual Bias During Geometric Scaling of Scatterplots","type":"publications"},{"authors":["Ying Zhao","Xiaobo Luo","Xiaoru Lin","Hairong Wang","Xiaoyan Kui","Fangfang Zhou","Jinsong Wang","Yi Chen","Wei Chen"],"categories":[],"content":"","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"3ff15fab5675d7458c1e6f0b0bb53de8","permalink":"/publications/radiovis/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/publications/radiovis/","section":"publications","summary":"Traditional radio monitoring and management largely depend on radio spectrum data analysis, which requires considerable domain experience and heavy cognition effort and frequently results in incorrect signal judgment and incomprehensive situation awareness. Faced with increasingly complicated electromagnetic environments, radio supervisors urgently need additional data sources and advanced analytical technologies to enhance their situation awareness ability. This paper introduces a visual analytics approach for electromagnetic situation awareness. Guided by a detailed scenario and requirement analysis, we first propose a signal clustering method to process radio signal data and a situation assessment model to obtain qualitative and quantitative descriptions of the electromagnetic situations. We then design a two-module interface with a set of visualization views and interactions to help radio supervisors perceive and understand the electromagnetic situations by a joint analysis of radio signal data and radio spectrum data. Evaluations on real-world data sets and an interview with actual users demonstrate the effectiveness of our prototype system. Finally, we discuss the limitations of the proposed approach and provide future work directions.","tags":["Radio monitoring and management","radio signal data","radio spectrum data","situation awareness","visual analytics"],"title":"Visual Analytics for Electromagnetic Situation Awareness in Radio Monitoring and Management","type":"publications"},{"authors":["Minfeng Zhu","Pingbo Pan","Wei Chen","Yi Yang"],"categories":[],"content":"","date":1554191162,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554191162,"objectID":"f5b26a577d3f82a4025a331143d94306","permalink":"/publications/dmgan/","publishdate":"2019-04-02T15:46:02+08:00","relpermalink":"/publications/dmgan/","section":"publications","summary":"In this paper, we focus on generating realistic images from text descriptions. Current methods first generate an initial image with rough shape and color, and then refine the initial image to a high-resolution one. Most existing text-to-image synthesis methods have two main problems. (1) These methods depend heavily on the quality of the initial images. If the initial image is not well initialized, the following processes can hardly refine the image to a satisfactory quality. (2) Each word contributes a different level of importance when depicting different image contents, however, unchanged text representation is used in existing image refinement processes. In this paper, we propose the Dynamic Memory Generative Adversarial Network (DM-GAN) to generate high-quality images. The proposed method introduces a dynamic memory module to refine fuzzy image contents, when the initial images are not well generated. A memory writing gate is designed to select the important text information based on the initial image content, which enables our method to accurately generate images from the text description. We also utilize a response gate to adaptively fuse the information read from the memories and the image features. We evaluate the DM-GAN model on the Caltech-UCSD Birds 200 dataset and the Microsoft Common Objects in Context dataset. Experimental results demonstrate that our DM-GAN model performs favorably against the state-of-the-art approaches.","tags":["Generative Adversarial Networks","Text-to-Image Synthesis"],"title":"DM-GAN: Dynamic Memory Generative Adversarial Networks for Text-to-Image Synthesis.","type":"publications"},{"authors":["Minfeng Zhu","Wei Chen","Jiazhi Xia","Yuxin Ma","Yankong Zhang","Yuetong Luo","Zhaosong Huang","Liangjun Liu"],"categories":[],"content":"","date":1552296004,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552296004,"objectID":"6b8742d95221bd64017ace6014deab59","permalink":"/publications/location2vec/","publishdate":"2019-03-11T17:20:04+08:00","relpermalink":"/publications/location2vec/","section":"publications","summary":"Understanding the relationship between urban locations is an essential task in urban planning and transportation management. Whereas prior works have focused on studying urban locations by aggregating location-based properties, our scheme preserves the mutual influence between urban locations and mobility behavior, and thereby enables situation-aware exploration of urban regions. By leveraging word embedding techniques, we encode urban locations with a vectorized representation while retaining situational awareness. Specifically, we design a spatial embedding algorithm that is precomputed by incorporating the interactions between urban locations and moving objects. To explore our proposed technique, we have designed and implemented a web-based visual exploration system that supports the comprehensive analysis of human mobility, location functionality, and traffic assessment by leveraging the proposed visual representation. Case studies demonstrate the effectiveness of our approach.","tags":["Human mobility","word embedding","urban computing","spatio-temporal data","visual exploration"],"title":"location2vec: a situation-aware representation for visual exploration of urban locations","type":"publications"},{"authors":["Farah Kamw","Shamal AL-Dohuki","Ye Zhao","Thomas Eynon","David Sheets","Jing Yang","Xinyue Ye","Wei Chen"],"categories":[],"content":"","date":1547596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547596800,"objectID":"7911625ab12d05ea9b4d3aa40a2fe2ec","permalink":"/publications/accessibility_modeling/","publishdate":"2019-01-16T00:00:00Z","relpermalink":"/publications/accessibility_modeling/","section":"publications","summary":"In modern cities, service providers want to identify the regions that are hard to reach from multiple fire stations, a citizen wants to meet with friends in a restaurant close to everyone, and administrators want to find whether an area far from two bus stations needs a new one. Such tasks involve studying the dynamic accessibility of the urban structures over multiple geospatial and temporal constraints, which is an important topic in geographical sciences and urban transportation. In this paper, we present a new computational model and a visualization system that help domain users to interactively study the jointly constrained accessible regions, street segments, and Points of Interest (POIs). In particular, Urban Structure Accessibility Visualization system is built upon a new Min-Max Joint Set model, where specifically designed set operations not only represent the accessible regions but also compute the minimum and maximum access times to urban structures from the joint constraints. The computation and visualization are supported by a new graph model that accommodates the real-world dynamic traffic situation and the geographical settings of urban street segments and POIs. The visualization system allows the users to conveniently construct and manage accessible regions and visually explore the urban structures inside them.","tags":["Urban accessibility","urban trajectories","visual analytics","geo-visualization"],"title":"Urban Structure Accessibility Modeling and Visualization for Joint Spatiotemporal Constraints.","type":"publications"},{"authors":["Zhaosong Huang","Yafeng Lu","Elizabeth Mack","Wei Chen","Ross Maciejewski"],"categories":[],"content":"","date":1546415162,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546415162,"objectID":"a6286a627a9325d263a27d4f54a47659","permalink":"/publications/geouncertainty/","publishdate":"2019-01-02T15:46:02+08:00","relpermalink":"/publications/geouncertainty/","section":"publications","summary":"The choropleth map is an essential tool for spatial data analysis. However, the underlying attribute values of a spatial unit greatly influence the statistical analyses and map classification procedures when generating a choropleth map. If the attribute values incorporate a range of uncertainty, a critical task is determining how much the uncertainty impacts both the map visualization and the statistical analysis. In this paper, we present a visual analytics system that enhances our understanding of the impact of attribute uncertainty on data visualization and statistical analyses of these data. Our system consists of a parallel coordinates-based uncertainty specification view, an impact river and impact matrix visualization for region-based and simulation-based analysis, and a dual-choropleth map and t-SNE plot for visualizing the changes in classification and spatial autocorrelation over the range of uncertainty in the attribute values. We demonstrate our system through three use cases illustrating the impact of attribute uncertainty in geographic analysis.","tags":["geospatial analysis","uncertainty","visualization","choropleth"],"title":"Exploring the Sensitivity of Choropleths under Attribute Uncertainty.","type":"publications"},{"authors":["Jie Li","Siming Chen","Wei Chen","Gennady Andrienko","Natalia Andrienko"],"categories":[],"content":"","date":1542672000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542672000,"objectID":"c3832071517ff6936de3d37381da35f7","permalink":"/publications/semantics_cube/","publishdate":"2018-11-20T00:00:00Z","relpermalink":"/publications/semantics_cube/","section":"publications","summary":"We propose an approach to analyzing data in which texts are associated with spatial and temporal references with the aim to understand how the text semantics vary over space and time. To represent the semantics, we apply probabilistic topic modeling. After extracting a set of topics and representing the texts by vectors of topic weights, we aggregate the data into a data cube with the dimensions corresponding to the set of topics, the set of spatial locations (e.g., regions), and the time divided into suitable intervals according to the scale of the planned analysis. Each cube cell corresponds to a combination (topic, location, time interval) and contains aggregate measures characterizing the subset of the texts concerning this topic and having the spatial and temporal references within these location and interval. Based on this structure, we systematically describe the space of analysis tasks on exploring the interrelationships among the three heterogeneous information facets, semantics, space, and time. We introduce the operations of projecting and slicing the cube, which are used to decompose complex tasks into simpler subtasks. We then present a design of a visual analytics system intended to support these subtasks. To reduce the complexity of the user interface, we apply the principles of structural, visual, and operational uniformity while respecting the specific properties of each facet. The aggregated data are represented in three parallel views corresponding to the three facets and providing different complementary perspectives on the data. The views have similar look-and-feel to the extent allowed by the facet specifics. Uniform interactive operations applicable to any view support establishing links between the facets. The uniformity principle is also applied in supporting the projecting and slicing operations on the data cube. We evaluate the feasibility and utility of the approach by applying it in two analysis scenarios using geolocated social media data for studying people's reactions to social and natural events of different spatial and temporal scales.","tags":["spatiotemporal visualization","semantic visualization","data cube","interactive exploration","visual analytics"],"title":"Semantics-Space-Time Cube: A Conceptual Framework for Systematic Analysis of Texts in Space and Time.","type":"publications"},{"authors":["Wei Chen","Zhaosong Huang","Feiran Wu","Minfeng Zhu","Huihua Guan","Ross Maciejewski"],"categories":[],"content":"","date":1541144762,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541144762,"objectID":"4c990c2993787880b30fd3c79f42d30c","permalink":"/publications/vaud/","publishdate":"2018-11-02T15:46:02+08:00","relpermalink":"/publications/vaud/","section":"publications","summary":"Urban data is massive, heterogeneous, and spatio-temporal, posing a substantial challenge for visualization and analysis. In this paper, we design and implement a novel visual analytics approach, Visual Analyzer for Urban Data (VAUD), that supports the visualization, querying, and exploration of urban data. Our approach allows for cross-domain correlation from multiple data sources by leveraging spatial-temporal and social inter-connectedness features. Through our approach, the analyst is able to select, filter, aggregate across multiple data sources and extract information that would be hidden to a single data subset. To illustrate the effectiveness of our approach, we provide case studies on a real urban dataset that contains the cyber-, physical-, and socialinformation of 14 million citizens over 22 days.","tags":["Urban data","Visual Analysis","Visual Reasoning","Heterogeneous","Spatio-temporal"],"title":"VAUD: A visual analysis approach for exploring spatio-temporal urban data.","type":"publications"},{"authors":["Xumeng Wang","Tianlong Gu","Xiwen Cai","Tianyi Lao","Wenlong Chen","Yingcai Wu","Jinhui Yu","Wei Chen"],"categories":[],"content":"","date":1540799162,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540799162,"objectID":"376f0752de29b54bfadf40a02d3307c2","permalink":"/publications/user-study/","publishdate":"2018-10-29T15:46:02+08:00","relpermalink":"/publications/user-study/","section":"publications","summary":"Visual analysis is widely applied to study human mobility due to the ability in integrating contextual information multiple data sources. Analyzing trajectory data through visualization improves the efficiency and accuracy of the analysis, yet may induce exposure of the location privacy. To balance the location privacy and analysis effectiveness, this work focuses on the behaviors of different geo-based contexts in the process of trajectory interpretation. Three types of geo-based contexts are identified after surveying 94 related literatures. We further conduct experiments to investigate their capability by evaluating how they benefit the analysis, and whether they lead to the location privacy exposure. Finally, we report and discuss interesting findings, and provide guidelines to the design of privacypreserving analysis approaches for human periodic trajectories.","tags":["Periodic Trajectory","Location Privacy","Evaluation","Geo-based Context"],"title":"A User Study on the Capability of Three Geo-Based Features in Analyzing and Locating Trajectories.","type":"publications"},{"authors":["Xumeng Wang"],"categories":null,"content":"","date":1540431380,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540431380,"objectID":"f29d256d89686c3dd37c4d29ae1f932d","permalink":"/talks/2018vis-xumeng/","publishdate":"2019-06-29T20:05:20+08:00","relpermalink":"/talks/2018vis-xumeng/","section":"talks","summary":"VAST 2018: GraphProtector: a Visual Interface for Employing and Assessing Multiple Privacy Preserving Graph Algorithms","tags":["VIS2018"],"title":"Xumeng Wang presenting GraphProtector at IEEE VAST 2018","type":"talks"},{"authors":["Dongming Han","Jiacheng Pan"],"categories":null,"content":"","date":1540352600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540352600,"objectID":"0565e1696f1f86c9295dcb634ebd2f70","permalink":"/talks/2018vis-han-pan/","publishdate":"2019-06-29T20:05:20+08:00","relpermalink":"/talks/2018vis-han-pan/","section":"talks","summary":"InfoVis 2018: Structure-Based Suggestive Exploration: A New Approach for Effective Exploration of Large Networks","tags":["VIS2018"],"title":"Dongming Han and Jiacheng Pan presenting Structure-Based Suggestive Exploration at IEEE InfoVis 2018","type":"talks"},{"authors":["Yuxin Ma","Anthony K. H. Tung","Wei Wang","Xiang Gao","Zhigeng Pan","Wei Chen"],"categories":[],"content":"","date":1539302400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539302400,"objectID":"e394c6152292e0efc113576dde1032cf","permalink":"/publications/scatternet/","publishdate":"2018-10-12T00:00:00Z","relpermalink":"/publications/scatternet/","section":"publications","summary":"Similarity measuring methods are widely adopted in a broad range of visualization applications. In this work, we address the challenge of representing human perception in the visual analysis of scatterplots by introducing a novel deep-learning-based approach, ScatterNet, captures perception-driven similarities of such plots. The approach exploits deep neural networks to extract semantic features of scatterplot images for similarity calculation. We create a large labeled dataset consisting of similar and dissimilar images of scatterplots to train the deep neural network. We conduct a set of evaluations including performance experiments and a user study to demonstrate the effectiveness and efficiency of our approach. The evaluations confirm that the learned features capture the human perception of scatterplot similarity effectively. We describe two scenarios to show how ScatterNet can be applied in visual analysis applications.","tags":["Scatterplot","similarity measuring","deep learning","visualization","visual exploration"],"title":"ScatterNet: A Deep Subjective Similarity Model for Visual Analysis of Scatterplots.","type":"publications"},{"authors":["Hongsen Liao","Yingcai Wu","Li Chen","Wei Chen"],"categories":[],"content":"","date":1535760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535760000,"objectID":"7d64421225e0e8c9bbf41e9fa36f88a3","permalink":"/publications/cluster-based/","publishdate":"2018-09-01T00:00:00Z","relpermalink":"/publications/cluster-based/","section":"publications","summary":"The use of scatterplots is an important method for multivariate data visualization. The point distribution on the scatterplot, along with variable values represented by each point, can help analyze underlying patterns in data. However, determining the multivariate data variation on a scatterplot generated using projection methods, such as multidimensional scaling, is difficult. Furthermore, the point distribution becomes unclear when the data scale is large and clutter problems occur. These conditions can significantly decrease the usability of scatterplots on multivariate data analysis. In this study, we present a cluster-based visual abstraction method to enhance the visualization of multivariate scatterplots. Our method leverages an adapted multilabel clustering method to provide abstractions of high quality for scatterplots. An image-based method is used to deal with large scale data problem. Furthermore, a suite of glyphs is designed to visualize the data at different levels of detail and support data exploration. The view coordination between the glyph-based visualization and the table lens can effectively enhance the multivariate data analysis. Through numerical evaluations for data abstraction quality, case studies and a user study, we demonstrate the effectiveness and usability of the proposed techniques for multivariate data analysis on scatterplots.","tags":["Data abstraction","scatterplot","glyph visualization","multilabel optimization"],"title":"Cluster-based Visual Abstraction for Multivariate Scatterplots","type":"publications"},{"authors":["Ying Zhao","Feng Luo","Minghui Chen","Yingchao Wang","Jiazhi Xia","Fangfang Zhou","Yunhai Wang","Yi Chen","Wei Chen"],"categories":[],"content":"","date":1534751162,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534751162,"objectID":"925c2614ef75ad67960c0c8878731f89","permalink":"/publications/fuzzyclustering/","publishdate":"2018-08-20T15:46:02+08:00","relpermalink":"/publications/fuzzyclustering/","section":"publications","summary":"Fuzzy clustering assigns a probability of membership for a datum to a cluster, which veritably reflects real-world clustering scenarios but significantly increases the complexity of understanding fuzzy clusters. Many studies have demonstrated that visualization techniques for multi-dimensional data are beneficial to understand fuzzy clusters. However, no empirical evidence exists on the effectiveness and efficiency of these visualization techniques in solving analytical tasks featured by fuzzy clusters. In this paper, we conduct a controlled experiment to evaluate the ability of fuzzy clusters analysis to use four multi-dimensional visualization techniques, namely, parallel coordinate plot, scatterplot matrix, principal component analysis, and Radviz. First, we define the analytical tasks and their representative questions specific to fuzzy clusters analysis. Then, we design objective questionnaires to compare the accuracy, time, and satisfaction in using the four techniques to solve the questions. We also design subjective questionnaires to collect the experience of the volunteers with the four techniques in terms of ease of use, informativeness, and helpfulness. With a complete experiment process and a detailed result analysis, we test against four hypotheses that are formulated on the basis of our experience, and provide instructive guidance for analysts in selecting appropriate and efficient visualization techniques to analyze fuzzy clusters.","tags":["Data visualization","Task analysis","Principal component analysis","Visualization","Encoding","Clustering algorithms","Correlation"],"title":"Evaluating Multi-Dimensional Visualizations for Understanding Fuzzy Clusters.","type":"publications"},{"authors":["Xumeng Wang","Wei Chen","Jia-Kai Chou","Chris Bryan","Huihua Guan","Wenlong Chen","Rusheng Pan","Kwan-Liu Ma"],"categories":[],"content":"","date":1534751162,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534751162,"objectID":"fb71c67bd0b2e47d8fb759c77ae362d1","permalink":"/publications/graphprotector/","publishdate":"2018-08-20T15:46:02+08:00","relpermalink":"/publications/graphprotector/","section":"publications","summary":"Analyzing social networks reveals the relationships between individuals and groups in the data. However, such analysis can also lead to privacy exposure (whether intentionally or inadvertently): leaking the real-world identity of ostensibly anonymous individuals. Most sanitization strategies modify the graph's structure based on hypothesized tactics that an adversary would employ. While combining multiple anonymization schemes provides a more comprehensive privacy protection, deciding the appropriate set of techniques-along with evaluating how applying the strategies will affect the utility of the anonymized results-remains a significant challenge. To address this problem, we introduce GraphProtector, a visual interface that guides a user through a privacy preservation pipeline. GraphProtector enables multiple privacy protection schemes which can be simultaneously combined together as a hybrid approach. To demonstrate the effectiveness of GraphPro tector, we report several case studies and feedback collected from interviews with expert users in various scenarios.","tags":["Graph privacy","K-Anonymity","Structural Features","Privacy Preservation"],"title":"GraphProtector: A Visual Interface for Employing and Assessing Multiple Privacy Preserving Graph Algorithms.","type":"publications"},{"authors":["Zhiguang Zhou","Linhao Meng","Cheng Tang","Ying Zhao","Zhiyong Guo","Miaoxin Hu","Wei Chen"],"categories":[],"content":"","date":1534751162,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534751162,"objectID":"c7adf5cc5986ac89c3513e1df6d9a625","permalink":"/publications/odflow/","publishdate":"2018-08-20T15:46:02+08:00","relpermalink":"/publications/odflow/","section":"publications","summary":"A variety of human movement datasets are represented in an Origin-Destination(OD) form, such as taxi trips, mobile phone locations, etc. As a commonly-used method to visualize OD data, flow map always fails to discover patterns of human mobility, due to massive intersections and occlusions of lines on a 2D geographical map. A large number of techniques have been proposed to reduce visual clutter of flow maps, such as filtering, clustering and edge bundling, but the correlations of OD flows are often neglected, which makes the simplified OD flow map present little semantic information. In this paper, a characterization of OD flows is established based on an analogy between OD flows and natural language processing (NPL) terms. Then, an iterative multi-objective sampling scheme is designed to select OD flows in a vectorized representation space. To enhance the readability of sampled OD flows, a set of meaningful visual encodings are designed to present the interactions of OD flows. We design and implement a visual exploration system that supports visual inspection and quantitative evaluation from a variety of perspectives. Case studies based on real-world datasets and interviews with domain experts have demonstrated the effectiveness of our system in reducing the visual clutter and enhancing correlations of OD flows.","tags":["Data visualization","Clutter","Geospatial analysis","Semantics","Mobile handsets","Correlation"],"title":"Visual Abstraction of Large Scale Geospatial Origin-Destination Movement Data.","type":"publications"},{"authors":["Yingcai Wu","Xiao Xie","Jiachen Wang","Dazhen Deng","Hongye Liang","Hui Zhang","Shoubin Cheng","Wei Chen"],"categories":[],"content":"","date":1534723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534723200,"objectID":"cbced8329b9bbee7b48eac75dd83c76d","permalink":"/publications/forvizor/","publishdate":"2018-08-20T00:00:00Z","relpermalink":"/publications/forvizor/","section":"publications","summary":"Regarded as a high-level tactic in soccer, a team formation assigns players different tasks and indicates their active regions on the pitch, thereby influencing the team performance significantly. Analysis of formations in soccer has become particularly indispensable for soccer analysts. However, formations of a team are intrinsically time-varying and contain inherent spatial information. The spatio-temporal nature of formations and other characteristics of soccer data, such as multivariate features, make analysis of formations in soccer a challenging problem. In this study, we closely worked with domain experts to characterize domain problems of formation analysis in soccer and formulated several design goals. We design a novel spatio-temporal visual representation of changes in team formation, allowing analysts to visually analyze the evolution of formations and track the spatial flow of players within formations over time. Based on the new design, we further design and develop ForVizor, a visual analytics system, which empowers users to track the spatio-temporal changes in formation and understand how and why such changes occur. With ForVizor, domain experts conduct formation analysis of two games. Analysis results with insights and useful feedback are summarized in two case studies.","tags":["Soccer data","formation analysis","spatio-temporal visualization"],"title":"ForVizor: Visualizing Spatio-Temporal Team Formations in Soccer.","type":"publications"},{"authors":["Wei Chen","Fangzhou Guo","Dongming Han","Jiacheng Pan","Xiaotao Nie","Jiazhi Xia","Xiaolong Zhang"],"categories":[],"content":"","date":1534723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534723200,"objectID":"de48c987f8795d89e3f5e038667eb7a8","permalink":"/publications/structure-based/","publishdate":"2018-08-20T00:00:00Z","relpermalink":"/publications/structure-based/","section":"publications","summary":"When analyzing a visualized network, users need to explore different sections of the network to gain insight. However, effective exploration of large networks is often a challenge. While various tools are available for users to explore the global and local features of a network, these tools usually require significant interaction activities, such as repetitive navigation actions to follow network nodes and edges. In this paper, we propose a structure-based suggestive exploration approach to support effective exploration of large networks by suggesting appropriate structures upon user request. Encoding nodes with vectorized representations by transforming information of surrounding structures of nodes into a high dimensional space, our approach can identify similar structures within a large network, enable user interaction with multiple similar structures simultaneously, and guide the exploration of unexplored structures. We develop a web-based visual exploration system to incorporate this suggestive exploration approach and compare performances of our approach under different vectorizing methods and networks. We also present the usability and effectiveness of our approach through a controlled user study with two datasets.","tags":["Graph","Embedding","Navigation","Network","Exploration","Structure","Search"],"title":"Structure-Based Suggestive Exploration: New Approach for Effective Exploration of Large Networks","type":"publications"},{"authors":["Tianlong Gu","Minfeng Zhu","Wei Chen","Zhaosong Huang","Ross Maciejewski","Liang Chang"],"categories":[],"content":"","date":1533195962,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533195962,"objectID":"05df07d7e4c09c3df275cbbe1c8b7970","permalink":"/publications/amtg/","publishdate":"2018-08-02T15:46:02+08:00","relpermalink":"/publications/amtg/","section":"publications","summary":"Modeling human mobility is a critical task in fields such as urban planning, ecology, and epidemiology. Given the current use of mobile phones, there is an abundance of data that can be used to create models of high reliability. Existing techniques can reveal the macro-patterns of crowd movement or analyze the trajectory of a person; however, they typically focus on geographical characteristics. This paper presents a graph-based approach for structuring crowd mobility transition over multiple granularities in the context of social behavior. The key to our approach is an adaptive data representation, the adaptive mobility transition graph, that is globally generated from citywide human mobility data by defining the temporal trends of human mobility and the interleaved transitions between different mobility patterns. We describe the design, creation and manipulation of the adaptive mobility transition graph and introduce a visual analysis system that supports the multi-faceted exploration of citywide human mobility patterns.","tags":["Timeline","Mobility","Mobility Transition","Mobility Patterns"],"title":"Structuring Mobility Transition With an Adaptive Graph Representation.","type":"publications"},{"authors":["Weifeng Chen","Wei Chen","Hujun Bao"],"categories":[],"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"4a23008feb45fe5123f847f0db320c96","permalink":"/publications/dichromats/","publishdate":"2018-06-01T08:00:00+08:00","relpermalink":"/publications/dichromats/","section":"publications","summary":"Color vision deficiency (CVD) affects a high percentage of the population worldwide. When seeing a volume visualization result, persons with CVD may be incapable of discriminating the classification information expressed in the image if the color transfer function or the color blending used in the direct volume rendering is not appropriate. Conventional methods used to address this problem adopt advanced image recoloring techniques to enhance the rendering results frame-by-frame; unfortunately, problematic perceptual results may still be generated. This paper proposes an alternative solution that complements the image recoloring scheme by reconfiguring the components of the direct volume rendering (DVR) pipeline. Our approach optimizes the mapped colors of a transfer function to simulate CVD-friendly effect that is generated by applying the image recoloring to the results with the initial transfer function. The optimization process has a low computational complexity, and only needs to be performed once for a given transfer function. To achieve detail-preserving and perceptually natural semi-transparent effects, we introduce a new color composition mode that works in the color space of dichromats. Experimental results and a pilot study demonstrates that our approach can yield dichromats-friendly and consistent volume visualization in real-time.","tags":["Dichromacy","direct volume rendering","volume classification","image recoloring"],"title":"An Efficient Direct Volume Rendering Algorithm for Dichromats.","type":"publications"},{"authors":["Deqing Li","Honghui Mei","Yi Shen","Shuang Su","Wenli Zhang","Junting Wang","Ming Zu","Wei Chen"],"categories":[],"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"9835e7e502270ef3760b236e0b54e14b","permalink":"/publications/echarts/","publishdate":"2018-06-01T08:00:00+08:00","relpermalink":"/publications/echarts/","section":"publications","summary":"While there have been a dozen of authoring systems and programming toolkits for visual design and development, users who do not have programming skills, such as data analysts or interface designers, still may feel cumbersome to efficiently implement a web-based visualization. In this paper, we present ECharts, an open-sourced, web-based, cross-platform framework that supports the rapid construction of interactive visualization. The motivation is driven by three goals: easy-to-use, rich built-in interactions, and high performance. The kernel of ECharts is a suite of declarative visual design language that customizes built-in chart types. The underlying streaming architecture, together with a high-performance graphics renderer based on HTML5 canvas, enables the high expandability and performance of ECharts. We report the design, implementation, and applications of ECharts with a diverse variety of examples. We compare the utility and performance of ECharts with C3.js, HighCharts, and Chart.js. Results of the experiments demonstrate the efficiency and scalability of our framework. Since the first release in June 2013, ECharts has iterated 63 versions, and attracted over 22,000 star counts and over 1700 related projects in the GitHub. ECharts is regarded as a leading visualization development tool in the world, and ranks the third in the GitHub visualization tab.","tags":["Visualization","Information Visualization","Visual Design","Web-based"],"title":"ECharts: A Declarative Framework for Rapid Construction of Web-based Visualization.","type":"publications"},{"authors":["Yunhai Wang","Wei Chen","Jian Zhang","Tingxing Dong","Guihua Shan","Xuebin Chi"],"categories":[],"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"d95f4ad5d336925ecb525b8af0481eb1","permalink":"/publications/gaussianmixture/","publishdate":"2018-06-01T08:00:00+08:00","relpermalink":"/publications/gaussianmixture/","section":"publications","summary":"The multidimensional transfer function is a flexible and effective tool for exploring volume data. However, designing an appropriate transfer function is a trial-and-error process and remains a challenge. In this paper, we propose a novel volume exploration scheme that explores volumetric structures in the feature space by modeling the space using the Gaussian mixture model (GMM). Our new approach has three distinctive advantages. First, an initial feature separation can be automatically achieved through GMM estimation. Second, the calculated Gaussians can be directly mapped to a set of elliptical transfer functions (ETFs), facilitating a fast pre-integrated volume rendering process. Third, an inexperienced user can flexibly manipulate the ETFs with the assistance of a suite of simple widgets, and discover potential features with several interactions. We further extend the GMM-based exploration scheme to time-varying data sets using an incremental GMM estimation algorithm. The algorithm estimates the GMM for one time step by using itself and the GMM generated from its previous steps. Sequentially applying the incremental algorithm to all time steps in a selected time interval yields a preliminary classification for each time step. In addition, the computed ETFs can be freely adjusted. The adjustments are then automatically propagated to other time steps. In this way, coherent user-guided exploration of a given time interval is achieved. Our GPU implementation demonstrates interactive performance and good scalability. The effectiveness of our approach is verified on several data sets.","tags":["Volume classification","volume rendering","Gaussian mixture model","time-varying data","temporal coherence"],"title":"Efficient Volume Exploration Using the Gaussian Mixture Model.","type":"publications"},{"authors":["Jin Huang","Zherong Pan","Guoning Chen","Wei Chen","Hujun Bao"],"categories":[],"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"a4c60a5fca98818b5ea41027970b709f","permalink":"/publications/lic/","publishdate":"2018-06-01T08:00:00+08:00","relpermalink":"/publications/lic/","section":"publications","summary":"Image-space line integral convolution (LIC) is a popular scheme for visualizing surface vector fields due to its simplicity and high efficiency. To avoid inconsistencies or color blur during the user interactions, existing approaches employ surface parameterization or 3D volume texture schemes. However, they often require expensive computation or memory cost, and cannot achieve consistent results in terms of both the granularity and color distribution on different scales. This paper introduces a novel image-space surface flow visualization approach that preserves the coherence during user interactions. To make the noise texture under different viewpoints coherent, we propose to precompute a sequence of mipmap noise textures in a coarse-to-fine manner for consistent transition, and map the textures onto each triangle with randomly assigned and constant texture coordinates. Further, a standard image-space LIC is performed to generate the flow texture. The proposed approach is simple and GPU-friendly, and can be easily combined with various texture-based flow visualization techniques. By leveraging viewpoint-dependent backward tracing and mipmap noise phase, our method can be incorporated with the image-based flow visualization (IBFV) technique for coherent visualization of unsteady flows. We demonstrate consistent and highly efficient flow visualization on a variety of data sets.","tags":["Flow visualization","mipmap","LIC","IBFV","surface flows","unsteady flows"],"title":"Image-Space Texture-Based Output-Coherent Surface Flow Visualization.","type":"publications"},{"authors":["Ming-Yuen Chan","Yingcai Wu","JWai-Ho Mak","Wei Chen","Huamin Qu"],"categories":[],"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"4115a36218b52b235a8dd3b04531476a","permalink":"/publications/perception-based/","publishdate":"2018-06-01T08:00:00+08:00","relpermalink":"/publications/perception-based/","section":"publications","summary":"The semi-transparent nature of direct volume rendered images is useful to depict layered structures in a volume. However, obtaining a semi-transparent result with the layers clearly revealed is difficult and may involve tedious adjustment on opacity and other rendering parameters. Furthermore, the visual quality of layers also depends on various perceptual factors. In this paper, we propose an auto-correction method for enhancing the perceived quality of the semi-transparent layers in direct volume rendered images. We introduce a suite of new measures based on psychological principles to evaluate the perceptual quality of transparent structures in the rendered images. By optimizing rendering parameters within an adaptive and intuitive user interaction process, the quality of the images is enhanced such that specific user requirements can be met. Experimental results on various datasets demonstrate the effectiveness and robustness of our method.","tags":["Direct volume rendering","image enhancement","layer perception"],"title":"Perception-Based Transparency Optimization for Direct Volume Rendering.","type":"publications"},{"authors":["Conglei Shi","Weiwei Cui","Shixia Liu","Panpan Xu","Wei Chen","Huamin Qu"],"categories":[],"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"7696b3b5fb41777a6eae9cd2d15c16a0","permalink":"/publications/rankexplorer/","publishdate":"2018-06-01T08:00:00+08:00","relpermalink":"/publications/rankexplorer/","section":"publications","summary":"For many applications involving time series data, people are often interested in the changes of item values over time as well as their ranking changes. For example, people search many words via search engines like Google and Bing every day. Analysts are interested in both the absolute searching number for each word as well as their relative rankings. Both sets of statistics may change over time. For very large time series data with thousands of items, how to visually present ranking changes is an interesting challenge. In this paper, we propose RankExplorer, a novel visualization method based on ThemeRiver to reveal the ranking changes. Our method consists of four major components: 1) a segmentation method which partitions a large set of time series curves into a manageable number of ranking categories; 2) an extended ThemeRiver view with embedded color bars and changing glyphs to show the evolution of aggregation values related to each ranking category over time as well as the content changes in each ranking category; 3) a trend curve to show the degree of ranking changes over time; 4) rich user interactions to support interactive exploration of ranking changes. We have applied our method to some real time series data and the case studies demonstrate that our method can reveal the underlying patterns related to ranking changes which might otherwise be obscured in traditional visualizations.","tags":["Time-series Data","Ranking change","Themeriver","Interaction Techniques"],"title":"RankExplorer: Visualization of Ranking Changes in Large Time Series Data.","type":"publications"},{"authors":["Fangzhou Guo","Tianlong Gu","Wei Chen","Feiran Wu","Qi Wang","Lei Shi","Huamin Qu"],"categories":[],"content":"","date":1524210362,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524210362,"objectID":"c7782098b05705175f8b14c0cda31e8d","permalink":"/publications/tcptree/","publishdate":"2018-04-20T15:46:02+08:00","relpermalink":"/publications/tcptree/","section":"publications","summary":"Discovering the correlations among variables of air quality data is challenging, because the correlation time series are long-lasting, multi-faceted, and information-sparse. In this article, we propose a novel visual representation, called Time-correlation-partitioning (TCP) tree, that compactly characterizes correlations of multiple air quality variables and their evolutions. A TCP tree is generated by partitioning the information-theoretic correlation time series into pieces with respect to the variable hierarchy and temporal variations, and reorganizing these pieces into a hierarchically nested structure. The visual exploration of a TCP tree provides a sparse data traversal of the correlation variations and a situation-aware analysis of correlations among variables. This can help meteorologists understand the correlations among air quality variables better. We demonstrate the efficiency of our approach in a real-world air quality investigation scenario.","tags":["Sensor","Multivariate time-series","Information Theory","Transfer Entropy"],"title":"Visual Exploration of Air Quality Data with A Time-Correlation Partitioning Tree Based on Information Theory.","type":"publications"},{"authors":["Wei Chen","Jing Xia","Xumeng Wang","Yi Wang","Jun Chen","Tianlong Gu"],"categories":[],"content":"","date":1519890362,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519890362,"objectID":"b61d61d6ab8b67f3a579abc71cb7dc94","permalink":"/publications/relationlines/","publishdate":"2018-03-01T15:46:02+08:00","relpermalink":"/publications/relationlines/","section":"publications","summary":"The increased accessibility of urban sensor data and the popularity of social network applications is enabling the discovery of crowd mobility and personal communication patterns. However, studying the egocentric relationships of an individual (i.e., the egocentric relations) can be very challenging because available data may refer to direct contacts, such as phone calls between individuals, or indirect contacts, such as paired location presence. In this paper, we develop methods to integrate three facets extracted from heterogeneous urban data (timelines, calls and locations) through a progressive visual reasoning and inspection scheme. Our approach uses a detect-and-filter scheme, such that, prior to visual refinement and analysis, a coarse detection is performed to extract the target individual and construct the timeline of the target. It then detects spatio-temporal co-occurrences or call-based contacts to develop the egocentric network of the individual. The filtering stage is enhanced with a line-based visual reasoning interface that facilitates flexible and comprehensive investigation of egocentric relationships and connections in terms of time, space and social networks. The integrated system, RelationLines, is demonstrated using a dataset that contains taxi GPS data, cell-base mobility data, mobile calling data, microblog data and POI data of a city with millions of citizens. We examine the effectiveness and efficiency of our system by three case studies and user review.","tags":["Location-based","Egocentric Relations","Visual Reasoning","Heterogeneous Urban Data","Timeline"],"title":"Relationlines: Visual Reasoning of Egocentric Relations from Heterogeneous Urban Data","type":"publications"},{"authors":["Jiazhi Xia","Fenjin Ye","Wei Chen","Yusi Wang","Weifeng Chen","Yuxin Ma","Anthony K.H. Tung"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"e4beb1e5af4eb81a9e29dd485c4297d7","permalink":"/publications/ldsscanner/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publications/ldsscanner/","section":"publications","summary":"Many approaches for analyzing a high-dimensional dataset assume that the dataset contains specific structures, e.g., clusters in linear subspaces or non-linear manifolds. This yields a trial-and-error process to verify the appropriate model and parameters. This paper contributes an exploratory interface that supports visual identification of low-dimensional structures in a high-dimensional dataset, and facilitates the optimized selection of data models and configurations. Our key idea is to abstract a set of global and local feature descriptors from the neighborhood graph-based representation of the latent low-dimensional structure, such as pairwise geodesic distance (GD) among points and pairwise local tangent space divergence (LTSD) among pointwise local tangent spaces (LTS). We propose a new LTSD-GD view, which is constructed by mapping LTSD and GD to the x axis and y axis using 1D multidimensional scaling, respectively. Unlike traditional dimensionality reduction methods that preserve various kinds of distances among points, the LTSD-GD view presents the distribution of pointwise LTS (x axis) and the variation of LTS in structures (the combination of x axis and y axis). We design and implement a suite of visual tools for navigating and reasoning about intrinsic structures of a high-dimensional dataset. Three case studies verify the effectiveness of our approach.","tags":["High-dimensional data","low-dimensional structure","subspace","manifold","visual exploration"],"title":"LDSScanner: Exploratory Analysis of Low-Dimensional Structures in High-Dimensional Datasets","type":"publications"},{"authors":["Xumeng Wang","Jia-Kai Chou","Wei Chen","Huihua Guan","Wenlong Chen","Tianyi Lao Kwan-Liu Ma"],"categories":[],"content":"","date":1503992762,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1503992762,"objectID":"4ce6ff53e6160df11abe1a92de0f60b7","permalink":"/publications/utilityaware/","publishdate":"2017-08-29T15:46:02+08:00","relpermalink":"/publications/utilityaware/","section":"publications","summary":"Sharing data for public usage requires sanitization to prevent sensitive information from leaking. Previous studies have presented methods for creating privacy preserving visualizations. However, few of them provide sufcient feedback to users on how much utility is reduced (or preserved) during such a process. To address this, we design a visual interface along with a data manipulation pipeline that allows users to gauge utility loss while interactively and iteratively handling privacy issues in their data. Widely known and discussed types of privacy models, i.e., syntactic anonymity and differential privacy, are integrated and compared under different use case scenarios. Case study results on a variety of examples demonstrate the effectiveness of our approach.","tags":["Privacy Preservating Visualization","Utility Aware Anonymization","Syntactic Anonymity","Differential Privacy"],"title":"A Utility-aware Visual Approach for Anonymizing Multi-attribute Tabular Data.","type":"publications"},{"authors":["Fei Wang","Wei Chen","Ye Zhao","Tianyu Gu","Siyuan Gao","Hujun Bao"],"categories":[],"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"7d5e2cdbcb1433aac91ca6052db093a1","permalink":"/publications/populationmobilitypatterns/","publishdate":"2017-08-01T00:00:00Z","relpermalink":"/publications/populationmobilitypatterns/","section":"publications","summary":"Thanks to the ubiquitous cell phone use, we have never been so close to uncover population mobility patterns in urban area. While some researches utilize cellphone call records to mine population patterns, few works aim to depict population movement in adaptively spatial and temporal representations, i.e., from a community, a district in the city over an hour, a day to a week. In this paper, we construct a system which deciphers, transforms, queries, and visualizes the records from the millions of users in a city. In particular, we design a data structure, namely MobiHash, which collects phone call records over base stations and indexes them by utilizing a Voronoi division of the urban space. MobiHash supports responsive data queries so that users can interactively retrieve trajectories reflecting population flows in areas of interest. Moreover, population movement is represented as vector fields to reduce visual clutter and occlusions. Because of sparse moving points, a novel radiation model is proposed to interpolate population passing zones. Case studies and experts' feedback validate the utility and efficiency by comparing population moving patterns in different times by using our system.","tags":["Population Mobility Pattern","Visual Query","Flow Visualization","Cell Phone Data"],"title":"Adaptively Exploring Population Mobility Patterns in Flow Visualization","type":"publications"},{"authors":["Gennady Andrienko","Natalia Andrienko","Wei Chen","Ross Maciejewski","Ye Zhao"],"categories":[],"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"08398c605596292829f0fbe235b27f1d","permalink":"/publications/mobilityandtransportation/","publishdate":"2017-08-01T00:00:00Z","relpermalink":"/publications/mobilityandtransportation/","section":"publications","summary":"Many cities and countries are now striving to create intelligent transportation systems that utilize the current abundance of multisource and multiform data related to the functionality and the use of transportation infrastructure to better support human mobility, interests, and lifestyles. Such intelligent transportation systems aim to provide novel services that can enable transportation consumers and managers to be better informed and make safer and more efficient use of the infrastructure. However, the transportation domain is characterized by both complex data and complex problems, which calls for visual analytics approaches. The science of visual analytics is continuing to develop principles, methods, and tools to enable synergistic work between humans and computers through interactive visual interfaces. Such interfaces support the unique capabilities of humans (such as the flexible application of prior knowledge and experiences, creative thinking, and insight) and couple these abilities with machines' computational strengths, enabling the generation of new knowledge from large and complex data. In this paper, we describe recent developments in visual analytics that are related to the study of movement and transportation systems and discuss how visual analytics can enable and improve the intelligent transportation systems of the future. We provide a survey of literature from the visual analytics domain and organize the survey with respect to the different types of transportation data, movement and its relationship to infrastructure and behavior, and modeling and planning. We conclude with lessons learned and future directions, including social transportation, recommender systems, and policy implications.","tags":["Data visualization","graphical user interfaces","interactive systems"],"title":"Visual Analytics of Mobility and Transportation: State of the Art and Further Research Directions.","type":"publications"},{"authors":["Jing Xia","Wei Chen","Yumeng Hou","Xinxin Huang","Wanqi Hu","David S.Ebert"],"categories":[],"content":"","date":1490227200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1490227200,"objectID":"e2340f6cec99f24d86a382e2a8ddc8c2","permalink":"/publications/dimscanner/","publishdate":"2017-03-23T00:00:00Z","relpermalink":"/publications/dimscanner/","section":"publications","summary":"Exploring multi-dimensional datasets can be cumbersome if data analysts have little knowledge about the data. Various dimension relation inspection tools and dimension exploration tools have been proposed for efficient data examining and understanding. However, the needed workload varies largely with respect to data complexity and user expertise, which can only be reduced with rich background knowledge over the data. In this paper we address the workload challenge with a data structuring and exploration scheme that affords dimension relation detection and that serves as the background knowledge for further investigation. We contribute a novel data structuring scheme that leverages an information-theoretic view structuring algorithm to uncover information-aware relations among different data views, and thereby discloses redundancy and other relation patterns among dimensions. The integrated system, DimScanner, empowers analysts with rich user controls and assistance widgets to interactively detect the relations of multi-dimensional data.","tags":["Correlation","Exploration"],"title":"DimScanner: A Relation-based Visual Exploration Approach Towards Data Dimension Inspection.","type":"publications"},{"authors":["Jing Xia","Yumeng Hou","Victor Chen","Cheryl Qian","David S. Ebert","Wei Chen"],"categories":[],"content":"","date":1489536000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1489536000,"objectID":"d00c652282507d86052993786603bc60","permalink":"/publications/visualizing-rank-time-series-of-wikipedia-top-viewed-pages/","publishdate":"2017-03-15T00:00:00Z","relpermalink":"/publications/visualizing-rank-time-series-of-wikipedia-top-viewed-pages/","section":"publications","summary":"Visual clutter is a common challenge when visualizing large rank time series data. WikiTopReader, a reader of Wikipedia page rank, lets users explore connections among top-viewed pages by connecting page-rank behaviors with page-link relations. Such a combination enhances the unweighted Wikipedia page-link network and focuses attention on the page of interest. A set of user evaluations shows that the system effectively represents evolving ranking patterns and page-wise correlation.","tags":["computer graphics","rank time series","page view","page link","visualization"],"title":"Visualizing Rank Time Series of Wikipedia Top Viewed Pages.","type":"publications"},{"authors":["Panpan Xu","Honghui Mei","Liu Ren","Wei Chen"],"categories":[],"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"6778fcfac30bfc65146245f5cf459dc7","permalink":"/publications/vidx/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publications/vidx/","section":"publications","summary":"Visual analytics plays a key role in the era of connected industry (or industry 4.0, industrial internet) as modern machines and assembly lines generate large amounts of data and effective visual exploration techniques are needed for troubleshooting, process optimization, and decision making. However, developing effective visual analytics solutions for this application domain is a challenging task due to the sheer volume and the complexity of the data collected in the manufacturing processes. We report the design and implementation of a comprehensive visual analytics system, ViDX. It supports both real-time tracking of assembly line performance and historical data exploration to identify inefficiencies, locate anomalies, and form hypotheses about their causes and effects. The system is designed based on a set of requirements gathered through discussions with the managers and operators from manufacturing sites. It features interlinked views displaying data at different levels of detail. In particular, we apply and extend the Marey's graph by introducing a time-aware outlier-preserving visual aggregation technique to support effective troubleshooting in manufacturing processes. We also introduce two novel interaction techniques, namely the quantiles brush and samples brush, for the users to interactively steer the outlier detection algorithms. We evaluate the system with example use cases and an in-depth user interview, both conducted together with the managers and operators from manufacturing plants. The result demonstrates its effectiveness and reports a successful pilot application of visual analytics for manufacturing in smart factories.","tags":["Temporal Data","Marey’s Graph","Visual Analytics","Manufacturing","Smart Factory","Connected Industry","Industry 4.0"],"title":"ViDX: Visual Diagnostics of Assembly Line Performance in Smart Factories","type":"publications"},{"authors":["Wei Chen","Tianyi Lao","Jing Xia","Xinxin Huang","Biao Zhu","Wanqi Hu","Huihua Guan"],"categories":[],"content":"","date":1474934400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1474934400,"objectID":"f809a9132b0cc16736457f35e3e9afa7","permalink":"/publications/gameflow/","publishdate":"2016-09-27T00:00:00Z","relpermalink":"/publications/gameflow/","section":"publications","summary":"Although basketball games have received broad attention, the forms of game reports and webcast are purely content-based cross-media: texts, videos, snapshots, and performance figures. Analytical narrations of games that seek to compose a complete game from heterogeneous datasets are challenging for general media producers because such a composition is time-consuming and heavily depends on domain experts. In particular, an appropriate analytical commentary of basketball games requires two factors, namely, rich context and domain knowledge, which includes game events, player locations, player profiles, and team profiles, among others. This type of analytical commentary elicits a timely and effective basketball game data visualization made up of different sources of media. Existing visualizations of basketball games mainly profile a particular aspect of the game. Therefore, this paper presents an expressive visualization scheme that comprehensively illustrates NBA games with three levels of details: a season level, a game level, and a session level. We reorganize a basketball game as a sequence of sessions to depict the game states and heated confrontations. We design and implement a live system that integrates multimedia NBA datasets: play-by-play text data, box score data, game video data, and action area data. We demonstrate the effectiveness of this scheme with case studies and user feedbacks.","tags":["Game","Video","Trajectory"],"title":"GameFlow: Narrative Visualization of NBA Basketball Games","type":"publications"},{"authors":["Shamal Al-Dohuki","Yingyu Wu","Farah Kamw","Jing Yang","Xin Li","Ye Zhao","Xinyue Ye","Wei Chen","Chao Ma","Fei Wang"],"categories":[],"content":"","date":1472083200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1472083200,"objectID":"0efdc555db7513fc32909d560e015036","permalink":"/publications/semantictraj/","publishdate":"2016-08-25T00:00:00Z","relpermalink":"/publications/semantictraj/","section":"publications","summary":"Massive taxi trajectory data is exploited for knowledge discovery in transportation and urban planning. Existing tools typically require users to select and brush geospatial regions on a map when retrieving and exploring taxi trajectories and passenger trips. To answer seemingly simple questions such as “What were the taxi trips starting from Main Street and ending at Wall Street in the morning?” or “Where are the taxis arriving at the Art Museum at noon typically coming from?”, tedious and time consuming interactions are usually needed since the numeric GPS points of trajectories are not directly linked to the keywords such as “Main Street”, “Wall Street”, and “Art Museum”. In this paper, we present SemanticTraj, a new method for managing and visualizing taxi trajectory data in an intuitive, semantic rich, and efficient means. With SemanticTraj, domain and public users can find answers to the aforementioned questions easily through direct queries based on the terms. They can also interactively explore the retrieved data in visualizations enhanced by semantic information of the trajectories and trips. In particular, taxi trajectories are converted into taxi documents through a textualization transformation process. This process maps GPS points into a series of street/POI names and pick-up/drop-off locations. It also converts vehicle speeds into user-defined descriptive terms. Then, a corpus of taxi documents is formed and indexed to enable flexible semantic queries over a text search engine. Semantic labels and meta-summaries of the results are integrated with a set of visualizations in a SemanticTraj prototype, which helps users study taxi trajectories quickly and easily. A set of usage scenarios are presented to show the usability of the system. We also collected feedback from domain experts and conducted a preliminary user study to evaluate the visual system.","tags":["Semantic","Taxi","Trajectory","Question","Urban","Transportation","Search"],"title":"SemanticTraj: A New Approach to Interacting with Massive Taxi Trajectories","type":"publications"},{"authors":["Chris Muelder","Biao Zhu","Wei Chen","Hongxin Zhang","Kwan-Liu Ma"],"categories":[],"content":"","date":1454284800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1454284800,"objectID":"8fde7b97cbc81f675c917289ee1c6b64","permalink":"/publications/visual-analysis-of-cloud-computing-performance-using-behavioral-lines/","publishdate":"2016-02-01T00:00:00Z","relpermalink":"/publications/visual-analysis-of-cloud-computing-performance-using-behavioral-lines/","section":"publications","summary":"Cloud computing is an essential technology to Big Data analytics and services. A cloud computing system is often comprised of a large number of parallel computing and storage devices. Monitoring the usage and performance of such a system is important for efficient operations, maintenance, and security. Tracing every application on a large cloud system is untenable due to scale and privacy issues. But profile data can be collected relatively efficiently by regularly sampling the state of the system, including properties such as CPU load, memory usage, network usage, and others, creating a set of multivariate time series for each system. Adequate tools for studying such large-scale, multidimensional data are lacking. In this paper, we present a visual based analysis approach to understanding and analyzing the performance and behavior of cloud computing systems. Our design is based on similarity measures and a layout method to portray the behavior of each compute node over time. When visualizing a large number of behavioral lines together, distinct patterns often appear suggesting particular types of performance bottleneck. The resulting system provides multiple linked views, which allow the user to interactively explore the data by examining the data or a selected subset at different levels of detail. Our case studies, which use datasets collected from two different cloud systems, show that this visual based approach is effective in identifying trends and anomalies of the systems.","tags":["Cloud computing","multidimensional data","performance visualization","visual analytics"],"title":"Visual Analysis of Cloud Computing Performance Using Behavioral Lines.","type":"publications"},{"authors":["Xinhu Zheng","Wei Chen","Pu Wang","Dayong Shen","Songhang Chen","Xiao Wang","Qingpeng Zhang","Liuqin Yang"],"categories":[],"content":"","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448928000,"objectID":"4a56ec95ff78e8944146804d59c65655","permalink":"/publications/big-data-in-social-transportation/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/publications/big-data-in-social-transportation/","section":"publications","summary":"Big data for social transportation brings us unprecedented opportunities for resolving transportation problems for which traditional approaches are not competent and for building the next-generation intelligent transportation systems. Although social data have been applied for transportation analysis, there are still many challenges. First, social data evolve with time and contain abundant information, posing a crucial need for data collection and cleaning. Meanwhile, each type of data has specific advantages and limitations for social transportation, and one data type alone is not capable of describing the overall state of a transportation system. Systematic data fusing approaches or frameworks for combining social signal data with different features, structures, resolutions, and precision are needed. Second, data processing and mining techniques, such as natural language processing and analysis of streaming data, require further revolutions in effective utilization of real-time traffic information. Third, social data are connected to cyber and physical spaces. To address practical problems in social transportation, a suite of schemes are demanded for realizing big data in social transportation systems, such as crowdsourcing, visual analysis, and task-based services. In this paper, we overview data sources, analytical approaches, and application systems for social transportation, and we also suggest a few future research directions for this new social transportation field.","tags":["Big data","social transportation","intelligent transportation system","data analytics","crowdsourcing"],"title":"Big Data in Social Transportation.","type":"publications"},{"authors":["Hongsen Liao","Li Chen","Yingcai Wu","Yunhai Wang","Huizhang","Wei Chen"],"categories":[],"content":"","date":1443657600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1443657600,"objectID":"7fe665f11c842bc1ac2142120cc3d8b2","permalink":"/publications/a-visual-voting-framework-for-weather-forecast-calibration/","publishdate":"2015-10-01T00:00:00Z","relpermalink":"/publications/a-visual-voting-framework-for-weather-forecast-calibration/","section":"publications","summary":"Numerical weather predictions have been widely used for weather forecasting. Many large meteorological centers are producing highly accurate ensemble forecasts routinely to provide effective weather forecast services. However, biases frequently exist in forecast products because of various reasons, such as the imperfection of the weather forecast models. Failure to identify and neutralize the biases would result in unreliable forecast products that might mislead analysts; consequently, unreliable weather predictions are produced. The analog method has been commonly used to overcome the biases. Nevertheless, this method has some serious limitations including the difficulties in finding effective similar past forecasts, the large search space for proper parameters and the lack of support for interactive, real-time analysis. In this study, we develop a visual analytics system based on a novel voting framework to circumvent the problems. The framework adopts the idea of majority voting to combine judiciously the different variants of analog methods towards effective retrieval of the proper analogs for calibration. The system seamlessly integrates the analog methods into an interactive visualization pipeline with a set of coordinated views that characterizes the different methods. Instant visual hints are provided in the views to guide users in finding and refining analogs. We have worked closely with the domain experts in the meteorological research to develop the system. The effectiveness of the system is demonstrated using two case studies. An informal evaluation with the experts proves the usability and usefulness of the system.","tags":["Weather forecast","analog method","calibration","majority voting","visual analytics"],"title":"A Visual Voting Framework for Weather Forecast Calibration.","type":"publications"},{"authors":["Haidong Chen","Song Zhang","Wei Chen","Honghui Mei","Jiawei Zhang","Huamin Qu"],"categories":[],"content":"","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"318e1eb47ea5232ca31db890c1eb2865","permalink":"/publications/uncertainty/","publishdate":"2015-09-01T00:00:00Z","relpermalink":"/publications/uncertainty/","section":"publications","summary":"This paper presents an efficient visualization and exploration approach for modeling and characterizing the relationships and uncertainties in the context of a multidimensional ensemble dataset. Its core is a novel dissimilarity-preserving projection technique that characterizes not only the relationships among the mean values of the ensemble data objects but also the relationships among the distributions of ensemble members. This uncertainty-aware projection scheme leads to an improved understanding of the intrinsic structure in an ensemble dataset. The analysis of the ensemble dataset is further augmented by a suite of visual encoding and exploration tools. Experimental results on both artificial and real-world datasets demonstrate the effectiveness of our approach.","tags":["Ensemble visualization","uncertainty quantification","uncertainty visualization","multidimensional data visualization"],"title":"Uncertainty-Aware Multidimensional Ensemble Data Visualization and Exploration","type":"publications"},{"authors":["Fei Wang","Wei Chen","Feiran Wu","Ye Zhao","Han Hong","Tianyu Gu","Long Wang","Ronghua Liang","Hujun Bao"],"categories":[],"content":"","date":1426464000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1426464000,"objectID":"dfb6276777ce086ab3298f068ef930cc","permalink":"/publications/visual_reasoning/","publishdate":"2015-03-16T00:00:00Z","relpermalink":"/publications/visual_reasoning/","section":"publications","summary":"Transport assessment plays a vital role in urban planning and traffic control, which are influenced by multi-faceted traffic factors involving road infrastructure and traffic flow. Conventional solutions can hardly meet the requirements and expectations of domain experts. In this paper we present a data-driven solution by leveraging a visual analysis system to evaluate the real traffic situations based on taxi trajectory data. A sketch-based visual interface is designed to support dynamic query and visual reasoning of traffic situations within multiple coordinated views. In particular, we propose a novel road-based query model for analysts to interactively conduct evaluation tasks. This model is supported by a bi-directional hash structure, TripHash, which enables real-time responses to the data queries over a huge amount of trajectory data. Case studies with a real taxi GPS trajectory dataset (\u003e 30GB) show that our system performs well for on-demand transport assessment and reasoning.","tags":["Roads","Trajectory","Global Positioning System","Topology","Visualization","Indexes"],"title":"A Visual Reasoning Approach for Data-driven Transport Assessment on Urban Road.","type":"publications"},{"authors":["Wenchao Wu","Yixian Zheng","Huamin Qu","Wei Chen","Eduard Groeller","Lionei Ni"],"categories":[],"content":"","date":1426464000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1426464000,"objectID":"132c565d06d53fc14b65a28ac86965be","permalink":"/publications/boundaryseer/","publishdate":"2015-03-16T00:00:00Z","relpermalink":"/publications/boundaryseer/","section":"publications","summary":"Boundary changes exist ubiquitously in our daily life. From the Antarctic ozone hole to the land desertification, and from the territory of a country to the area within one-hour reach from a downtown location, boundaries change over time. With a large number of time-varying boundaries recorded, people often need to analyze the changes, detect their similarities or differences, and find out spatial and temporal patterns of the evolution for various applications. In this paper, we present a comprehensive visual analytics system, BoundarySeer, to help users gain insight into the changes of boundaries. Our system consists of four major viewers: 1) a global viewer to show boundary groups based on their similarity and the distribution of boundary attributes such as smoothness and perimeter; 2) a region viewer to display the regions encircled by the boundaries and how they are affected by boundary changes; 3) a trend viewer to reveal the temporal patterns in the boundary evolution and potential spatio-temporal correlations; 4) a directional change viewer to encode movements of boundary segments in different directions. Quantitative analyses of boundaries (e.g., similarity measurement and adaptive clustering) and intuitive visualizations (e.g., density map and ThemeRiver) are integrated into these viewers, which enable users to explore boundary changes from different aspects and at different scales. Case studies with two real-world datasets have been carried out to demonstrate the effectiveness of our system.","tags":["Data visualization","Stability analysis","Market research","Visualization","Power system stability","Heating"],"title":"BoundarySeer: Visual Analysis of 2D Boundary Changes.","type":"publications"},{"authors":["Haidong Chen","Wei Chen","Honghui Mei","Zhiqi Liu","Kun Zhou","Weifeng Chen","Wentao Gu","Kwan-Liu Ma"],"categories":[],"content":"","date":1419984000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1419984000,"objectID":"668a2d8141cc3671653d05ae82d22a04","permalink":"/publications/visual-abstraction/","publishdate":"2014-12-31T00:00:00Z","relpermalink":"/publications/visual-abstraction/","section":"publications","summary":"Scatterplots are widely used to visualize scatter dataset for exploring outliers, clusters, local trends, and correlations. Depicting multi-class scattered points within a single scatterplot view, however, may suffer from heavy overdraw, making it inefficient for data analysis. This paper presents a new visual abstraction scheme that employs a hierarchical multi-class sampling technique to show a feature-preserving simplification. To enhance the density contrast, the colors of multiple classes are optimized by taking the multi-class point distributions into account. We design a visual exploration system that supports visual inspection and quantitative analysis from different perspectives. We have applied our system to several challenging datasets, and the results demonstrate the efficiency of our approach.","tags":["Scatterplot","overdraw reduction","sampling","visual abstraction"],"title":"Visual Abstraction and Exploration of Multi-class Scatterplots","type":"publications"},{"authors":["Cong Xie","Wei Chen","Xinxin Hunag","Yueqi Hu","Scott Barlowe","Jing Yang"],"categories":[],"content":"","date":1415232000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1415232000,"objectID":"98b67698abf75aed6791b4f35b89d509","permalink":"/publications/vaet/","publishdate":"2014-11-06T00:00:00Z","relpermalink":"/publications/vaet/","section":"publications","summary":"Previous studies on E-transaction time-series have mainly focused on finding temporal trends of transaction behavior. Interesting transactions that are time-stamped and situation-relevant may easily be obscured in a large amount of information. This paper proposes a visual analytics system, Visual Analysis of E-transaction Time-Series (VAET), that allows the analysts to interactively explore large transaction datasets for insights about time-varying transactions. With a set of analyst-determined training samples, VAET automatically estimates the saliency of each transaction in a large time-series using a probabilistic decision tree learner. It provides an effective time-of-saliency (TOS) map where the analysts can explore a large number of transactions at different time granularities. Interesting transactions are further encoded with KnotLines, a compact visual representation that captures both the temporal variations and the contextual connection of transactions. The analysts can thus explore, select, and investigate knotlines of interest. A case study and user study with a real E-transactions dataset (26 million records) demonstrate the effectiveness of VAET.","tags":["Decision trees","Feature extraction","Data visualization","Time series analysis","Visual analytics","Probabilistic logic","Time series analysis"],"title":"VAET: A Visual Analytics Approach for E-transactions Time-series.","type":"publications"},{"authors":["Rui Wang","Hujun Bao","Karla Bala","Xianjin Yang","Yazhen Yuan","Wei Chen"],"categories":[],"content":"","date":1414800000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1414800000,"objectID":"c690419b710bd699716b1b65e7207805","permalink":"/publications/automatic_shader/","publishdate":"2014-11-01T00:00:00Z","relpermalink":"/publications/automatic_shader/","section":"publications","summary":"In this paper, we present a new automatic shader simplification method using surface signal approximation. We regard the entire multi-stage rendering pipeline as a process that generates signals on surfaces, and we formulate the simplification of the fragment shader as a global simplification problem across multi-shader stages. Three new shader simplification rules are proposed to solve the problem. First, the code transformation rule transforms fragment shader code to other shader stages in order to redistribute computations on pixels up to the level of geometry primitives. Second, the surface-wise approximation rule uses high-order polynomial basis functions on surfaces to approximate pixel-wise computations in the fragment shader. These approximations are pre-cached and simplify computations at runtime. Third, the surface subdivision rule tessellates surfaces into smaller patches. It combines with the previous two rules to approximate pixel-wise signals at different levels of tessellations with different computation times and visual errors. To evaluate simplified shaders using these simplification rules, we introduce a new cost model that includes the visual quality, rendering time and memory consumption. With these simplification rules and the cost model, we present an integrated shader simplification algorithm that is capable of automatically generating variants of simplified shaders and selecting a sequence of preferable shaders. Results show that the sequence of selected simplified shaders balance performance, accuracy and memory consumption well.","tags":["TGPU shader","real-time rendering","shader simplification","surface signal approximation"],"title":"Automatic Shader Simplification using Surface Signal Approximation.","type":"publications"},{"authors":["Long Zhang","Ying He","Jiazhi Xia","Xuexiang Xie","Wei Chen"],"categories":[],"content":"","date":1285200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1285200000,"objectID":"d174c892b01b196820a2a5f8de1ca2cb","permalink":"/publications/real-timeshape/","publishdate":"2010-09-23T00:00:00Z","relpermalink":"/publications/real-timeshape/","section":"publications","summary":"This paper presents a novel object-space line drawing algorithm that can depict shapes with view-dependent feature lines in real time. Strongly inspired by the Laplacian-of-Gaussian (LoG) edge detector in image processing, we define Laplacian lines as the zero-crossing points of the Laplacian of the surface illumination. Compared to other view-dependent feature lines, Laplacian lines are computationally efficient because most expensive computations can be preprocessed. We further extend Laplacian lines to volumetric data and develop the algorithm to compute volumetric Laplacian lines without isosurface extraction. We apply the proposed Laplacian lines to a wide range of real-world models and demonstrate that Laplacian lines are more efficient than the existing computer generated feature lines, and can be used in interactive graphics applications.","tags":["Laplacian","Feature extraction"],"title":"Real-Time Shape Illustration Using Laplacian Lines","type":"publications"},{"authors":["Ross Maciejewski","Insoo Wu","Wei Chen","David S. Ebert"],"categories":[],"content":"","date":1257033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1257033600,"objectID":"b5a5fe566617bd963fbdb684e8c83718","permalink":"/publications/structuringfeaturespace/","publishdate":"2009-11-01T00:00:00Z","relpermalink":"/publications/structuringfeaturespace/","section":"publications","summary":"The use of multi-dimensional transfer functions for direct volume rendering has been shown to be an effective means of extracting materials and their boundaries for both scalar and multivariate data. The most common multi-dimensional transfer function consists of a two-dimensional (2D) histogram with axes representing a subset of the feature space (e.g., value vs. value gradient magnitude), with each entry in the 2D histogram being the number of voxels at a given feature space pair. Users then assign color and opacity to the voxel distributions within the given feature space through the use of interactive widgets (e.g., box, circular, triangular selection). Unfortunately, such tools lead users through a trial-and-error approach as they assess which data values within the feature space map to a given area of interest within the volumetric space. In this work, we propose the addition of non-parametric clustering within the transfer function feature space in order to extract patterns and guide transfer function generation. We apply a non-parametric kernel density estimation to group voxels of similar features within the 2D histogram. These groups are then binned and colored based on their estimated density, and the user may interactively grow and shrink the binned regions to explore feature boundaries and extract regions of interest. We also extend this scheme to temporal volumetric data in which time steps of 2D histograms are composited into a histogram volume. A three-dimensional (3D) density estimation is then applied, and users can explore regions within the feature space across time without adjusting the transfer function at each time step. Our work enables users to effectively explore the structures found within a feature space of the volume and provide a context in which the user can understand how these structures relate to their volumetric data. We provide tools for enhanced exploration and manipulation of the transfer function, and we show that the initial transfer function generation serves as a reasonable base for volumetric rendering, reducing the trial-and-error overhead typically found in transfer function design.","tags":["Volume rendering","kernel density estimation","transfer function design","temporal volume rendering"],"title":"Structuring Feature Space: A Non-Parametric Method for Volumetric Transfer Function Generation.","type":"publications"},{"authors":["Wei Chen","Zi’ang Ding","Song Zhang","Anna MacKay-Brandt","Stephen Correia","Huamin Qu","John Allen Crow","David F. Tate","Zhicheng Yan","Qunsheng Peng"],"categories":[],"content":"","date":1256256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1256256000,"objectID":"8bfc176b3cae7339b74dd2d8f204398b","permalink":"/publications/dti/","publishdate":"2009-10-23T00:00:00Z","relpermalink":"/publications/dti/","section":"publications","summary":"Visual exploration is essential to the visualization and analysis of densely sampled 3D DTI fibers in biological speciments, due to the high geometric, spatial, and anatomical complexity of fiber tracts. Previous methods for DTI fiber visualization use zooming, color-mapping, selection, and abstraction to deliver the characteristics of the fibers. However, these schemes mainly focus on the optimization of visualization in the 3D space where cluttering and occlusion make grasping even a few thousand fibers difficult. This paper introduces a novel interaction method that augments the 3D visualization with a 2D representation containing a low-dimensional embedding of the DTI fibers. This embedding preserves the relationship between the fibers and removes the visual clutter that is inherent in 3D renderings of the fibers. This new interface allows the user to manipulate the DTI fibers as both 3D curves and 2D embedded points and easily compare or validate his or her results in both domains. The implementation of the framework is GPU based to achieve real-time interaction. The framework was applied to several tasks, and the results show that our method reduces the user's workload in recognizing 3D DTI fibers and permits quick and accurate DTI fiber selection.","tags":["Diffusion Tensor Imaging","Fibers","Fiber Clustering","Visualization Interface"],"title":"A Novel Interface for Interactive Exploration of DTI Fibers.","type":"publications"},{"authors":["Wei Chen","Zhicheng Yan","Song Zhang","John Allen Crow","David S. Ebert","R. McLaughlin","K. Mullins","R. Cooper","Zi’ang Ding","Jun Liao"],"categories":[],"content":"","date":1246267204,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1246267204,"objectID":"23e1e2ad2de405a649a78821409232cb","permalink":"/publications/vimdti/","publishdate":"2009-06-29T17:20:04+08:00","relpermalink":"/publications/vimdti/","section":"publications","summary":"Medical illustration has demonstrated its effectiveness to depict salient anatomical features while hiding the irrelevant details. Current solutions are ineffective for visualizing fibrous structures such as muscle, because typical datasets (CT or MRI) do not contain directional details. In this paper, we introduce a new muscle illustration approach that leverages diffusion tensor imaging (DTI) data and example-based texture synthesis techniques. Beginning with a volumetric diffusion tensor image, we reformulate it into a scalar field and an auxiliary guidance vector field to represent the structure and orientation of a muscle bundle. A muscle mask derived from the input diffusion tensor image is used to classify the muscle structure. The guidance vector field is further refined to remove noise and clarify structure. To simulate the internal appearance of the muscle, we propose a new two-dimensional examplebased solid texture synthesis algorithm that builds a solid texture constrained by the guidance vector field. Illustrating the constructed scalar field and solid texture efficiently highlights the global appearance of the muscle as well as the local shape and structure of the muscle fibers in an illustrative fashion. We have applied the proposed approach to five example datasets (four pig hearts and a pig leg), demonstrating plausible illustration and expressiveness.","tags":["Illustrative Visualization","Diffusion Tensor Image","Muscle","Solid Texture Synthesis"],"title":"Volume Illustration of Muscle from Diffusion Tensor Images.","type":"publications"},{"authors":["Wei Chen","Liu Ren","Matthias Zwicker","Hanspter Pfister"],"categories":[],"content":"","date":1097366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1097366400,"objectID":"800d835a26d8eec92d236359693c1a24","permalink":"/publications/hardware/","publishdate":"2004-10-10T00:00:00Z","relpermalink":"/publications/hardware/","section":"publications","summary":"We present a hardware-accelerated adaptive EWA (elliptical weighted average) volume splatting algorithm. EWA splatting combines a Gaussian reconstruction kernel with a low-pass image filter for high image quality without aliasing artifacts or excessive blurring. We introduce a novel adaptive filtering scheme to reduce the computational cost of EWA splatting. We show how this algorithm can be efficiently implemented on modern graphics processing units (GPUs). Our implementation includes interactive classification and fast lighting. To accelerate the rendering we store splat geometry and 3D volume data locally in GPU memory. We present results for several rectilinear volume datasets that demonstrate the high image quality and interactive rendering speed of our method.","tags":["Direct volume rendering","volume splatting","EWA filter","hardware acceleration"],"title":"Hardware Accelerated Adaptive EWA Volume Splatting","type":"publications"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"79aecca6728ccedff543c98f727db897","permalink":"/papercollection/eurovis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/papercollection/eurovis/","section":"papercollection","summary":"This collection includes EuroVis from 2013: EuroVis2020 EuroVis2019 and before ","tags":null,"title":"EuroVis","type":"papercollection"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"10e7f170db713c8297668814a1b4a284","permalink":"/papercollection/ieeepvis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/papercollection/ieeepvis/","section":"papercollection","summary":"This collection includes IEEE Pacific Visualization from 2013. PVis2020 PVis2019 and before ","tags":null,"title":"IEEE Pacific Visualization","type":"papercollection"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"11cd8fd9ace2b8f9af0baae72b014fa8","permalink":"/papercollection/ieeevis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/papercollection/ieeevis/","section":"papercollection","summary":"This collection includes IEEE VIS and IEEE Visualization from 2003 and references of other paper collection maintainers: VIS2020 VIS2019 and before ","tags":null,"title":"IEEE VIS (VAST, InfoVis and SciVis)","type":"papercollection"}]